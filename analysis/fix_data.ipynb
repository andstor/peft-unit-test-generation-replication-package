{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import os\n",
    "parser_dir = pathlib.Path(os.path.dirname(os.path.abspath(\"__file__\"))) / 'java-universal-parser' / 'parser'\n",
    "sys.path.append(str(parser_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python3.JavaLexer import JavaLexer\n",
    "from python3.JavaParser import JavaParser\n",
    "\n",
    "from antlr4 import InputStream, CommonTokenStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from antlr4.error.ErrorListener import ErrorListener\n",
    "\n",
    "class MyErrorListener( ErrorListener ):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyErrorListener, self).__init__()\n",
    "\n",
    "    def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):\n",
    "        #print(\"line \" + str(line) + \":\" + str(column) + \" \" + msg, file=sys.stderr)\n",
    "        raise Exception(\"Oh no!!\", line, column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_syntax(code):\n",
    "    try:\n",
    "        input_stream = InputStream(code)\n",
    "        lexer = JavaLexer(input_stream)\n",
    "        lexer.removeErrorListeners() # remove default error listener\n",
    "        lexer.addErrorListener( MyErrorListener() )\n",
    "        stream = CommonTokenStream(lexer)\n",
    "        parser = JavaParser(stream)\n",
    "        parser.removeErrorListeners()\n",
    "        parser.addErrorListener( MyErrorListener() )\n",
    "        parser.compilationUnit()\n",
    "    except Exception as e:\n",
    "        line = e.args[1]\n",
    "        return False, line\n",
    "    return True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_brace = \"\\n    }\"\n",
    "outer_brace = \"\\n\\n}\"\n",
    "\n",
    "\n",
    "def get_brace_count(code, start_level=0):\n",
    "    count = start_level\n",
    "    for char in code:\n",
    "        if char == \"{\":\n",
    "            count += 1\n",
    "        if char == \"}\":\n",
    "            count -= 1\n",
    "    return count\n",
    "\n",
    "def balance_braces(code, start_level=0):\n",
    "    count = get_brace_count(code, start_level)\n",
    "    if count == 0:\n",
    "        #print(\"Well formed code\",)\n",
    "        return code\n",
    "    \n",
    "    lines = code.splitlines(keepends=True)\n",
    "    #print(\"Lines: \", len(lines))\n",
    "    for line in reversed(range(len(lines))):\n",
    "        candidate = \"\".join(lines)\n",
    "        count = get_brace_count(candidate, start_level)\n",
    "        #print(\"Count: \", count)\n",
    "        #print(\"Line idx: \", line)\n",
    "        if count == 0:\n",
    "            #print(\"0\")\n",
    "            return candidate\n",
    "        elif count == 1:\n",
    "            #print(\"1\")\n",
    "            candidate += outer_brace\n",
    "            return candidate\n",
    "        elif count == 2:\n",
    "            #print(\"2\")\n",
    "            candidate += inner_brace\n",
    "            candidate += outer_brace\n",
    "            return candidate\n",
    "        lines.pop()\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_formed_code(code):\n",
    "    success, bad_line = check_syntax(code)\n",
    "    lines = code.splitlines(keepends=False) \n",
    "    if not success:\n",
    "        # Trim the code to the bad line\n",
    "        lines = lines[:bad_line-1]\n",
    "\n",
    "        if len(lines) <= 1:\n",
    "            return False, None\n",
    "\n",
    "        else:\n",
    "            balanced_code = balance_braces(\"\\n\".join(lines))\n",
    "            success, bad_line = check_syntax(balanced_code)\n",
    "\n",
    "            if not success:\n",
    "                return well_formed_code(\"\\n\".join(lines))\n",
    "            else:\n",
    "                return True, balanced_code\n",
    "\n",
    "    else:\n",
    "        return True, code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "def well_formed_fix(code):\n",
    "    prefix = \"\"\"class dummyTest {\n",
    "        @Test\n",
    "        public void dummy() {\n",
    "    \"\"\"\n",
    "    code = prefix + code\n",
    "    try:\n",
    "        status, res = well_formed_code(code)\n",
    "    \n",
    "        if status:\n",
    "            res_trimmed = res.removeprefix(prefix)\n",
    "            return res_trimmed\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "            print(\"Error: \", e)\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def fix_data(dataset, method, model):\n",
    "    regex = r\"^(\\d+)-of-(\\d+).test.jsonl$\"\n",
    "    path= Path(\"../data/\" + str(dataset) +  \"/generated/\" + str(method) +  \"/\" + str(model))\n",
    "    print(path)\n",
    "    files = os.listdir(path)\n",
    "    print(files)\n",
    "    \n",
    "\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        matches = re.search(regex, file)\n",
    "        if matches:\n",
    "            print(\"Loading file: \", file)\n",
    "            df = pd.read_json(path / file, orient='records', lines=True, dtype=False)\n",
    "            dfs.append(df)\n",
    "\n",
    "\n",
    "    df = pd.concat(dfs, axis=0).set_index('id')\n",
    "    df.sort_index(inplace=True)\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "    total = df.shape[0]\n",
    "    df[\"fixed_prediction\"] = df[\"prediction\"].parallel_apply(well_formed_fix)\n",
    "    \n",
    "    # remove \"}\\n\\n}\" and none values\n",
    "    fixed_df = df[df[\"fixed_prediction\"]!=\"}\\n\\n}\"]\n",
    "    fixed_df = fixed_df[fixed_df[\"fixed_prediction\"].notnull()]\n",
    "\n",
    "    num_fixed = fixed_df.shape[0]\n",
    "\n",
    "    # save fixed predictions as jsonl\n",
    "    save_path= Path(\"../data/\" + str(dataset) +  \"/fixed/\" + str(method) +  \"/\" + str(model))\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    fixed_df.reset_index().to_json(save_path / \"00001-of-00001.jsonl\", orient='records', lines=True)\n",
    "    return (num_fixed, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "root, dirs, files = next(os.walk(\"../data\"))\n",
    "datasets = dirs\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset_path = Path(root, dataset, \"generated\")\n",
    "    \n",
    "    paths = []\n",
    "    for d_root, dirnames, filenames in os.walk(dataset_path):\n",
    "        # only keep \"output\" directories\n",
    "        path = Path(d_root)\n",
    "        #print(path.parts)\n",
    "        if len(path.parts) == 7:\n",
    "            paths.append(path)\n",
    "\n",
    "    paths = list(set(paths))\n",
    "\n",
    "    valid_syntax_data = {}\n",
    "    for path in tqdm(paths):\n",
    "        path = path.parts\n",
    "        method = path[4]\n",
    "        model = path[5] + \"/\" + path[6]\n",
    "        print(dataset, method, model)\n",
    "        unfixable, total = fix_data(dataset, method, model)\n",
    "\n",
    "        valid_syntax_data.setdefault(model, {})\n",
    "        percentage = unfixable / total if total > 0 else 0\n",
    "        valid_syntax_data[model][method] = percentage\n",
    "\n",
    "    valid_df = pd.DataFrame(valid_syntax_data)\n",
    "    valid_df.T.to_csv(Path(root, dataset, \"valid_syntax.csv\"), index_label = 'model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
