{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c2f66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc63bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'codegen-350M-multi': 'CodeGen-350M-multi',\n",
    "    'codegen2-1B_P': 'CodeGen2-1B',\n",
    "    'starcoder2-3b': 'StarCoder2-3B',\n",
    "    'codegen2-3_7B_P': 'CodeGen2-3.7B',\n",
    "    'CodeLlama-7b-hf': 'CodeLlama-7B',\n",
    "    'codegen2-7B_P': 'CodeGen2-7B',\n",
    "    'starcoder2-7b': 'StarCoder2-7B',\n",
    "    'starcoderbase': 'StarCoderBase',\n",
    "    'starcoder2-15b': 'StarCoder2-15B',\n",
    "    'codegen2-16B_P': 'CodeGen2-16B',\n",
    "}\n",
    "\n",
    "#datasets = ['methods2test_runnable', 'humaneval-x']\n",
    "datasets = {\n",
    "    'methods2test_runnable': '\\\\textsc{Methods2Test\\\\textsubscript{runnable}}',\n",
    "    'humaneval-x': '\\\\textsc{HumanEval-X\\\\textsubscript{java}}',\n",
    "}\n",
    "\n",
    "columns = ['valid_syntax', 'scores', 'coverage_runnable', 'coverage_instruction', 'coverage_branch']\n",
    "\n",
    "methods = {\n",
    "    'pre-trained': 'None',\n",
    "    'fine-tuning': 'Fine-tuning',\n",
    "    'lora': 'LoRA',\n",
    "    'ia3': '(IA)\\\\textsuperscript{3}',\n",
    "    'prompt-tuning': 'Prompt tuning',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e2dd0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeGen2-3.7B LoRA ValidSyntax\n",
    "#CodeGen2-1B Prompt tuning ValidSyntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "260bb1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "notable_results = [\n",
    "    ('methods2test_runnable', 'codegen2-1B_P', 'prompt-tuning', 'valid_syntax'),\n",
    "    ('methods2test_runnable', 'codegen2-3_7B_P', 'LoRA', 'valid_syntax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "588eedf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>ia3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>codegen-350M-multi</th>\n",
       "      <td>0</td>\n",
       "      <td>356,712,448</td>\n",
       "      <td>20,480</td>\n",
       "      <td>1,310,720</td>\n",
       "      <td>143,360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-1B_P</th>\n",
       "      <td>0</td>\n",
       "      <td>1,015,306,240</td>\n",
       "      <td>40,960</td>\n",
       "      <td>2,097,152</td>\n",
       "      <td>229,376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-3b</th>\n",
       "      <td>0</td>\n",
       "      <td>3,030,371,328</td>\n",
       "      <td>61,440</td>\n",
       "      <td>4,546,560</td>\n",
       "      <td>468,480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-3_7B_P</th>\n",
       "      <td>0</td>\n",
       "      <td>3,641,174,016</td>\n",
       "      <td>81,920</td>\n",
       "      <td>4,194,304</td>\n",
       "      <td>458,752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-7b-hf</th>\n",
       "      <td>0</td>\n",
       "      <td>6,738,546,688</td>\n",
       "      <td>81,920</td>\n",
       "      <td>8,388,608</td>\n",
       "      <td>614,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-7B_P</th>\n",
       "      <td>0</td>\n",
       "      <td>6,862,858,240</td>\n",
       "      <td>81,920</td>\n",
       "      <td>8,388,608</td>\n",
       "      <td>917,504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-7b</th>\n",
       "      <td>0</td>\n",
       "      <td>7,173,923,840</td>\n",
       "      <td>92,160</td>\n",
       "      <td>7,340,032</td>\n",
       "      <td>753,664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoderbase</th>\n",
       "      <td>0</td>\n",
       "      <td>15,517,456,384</td>\n",
       "      <td>122,880</td>\n",
       "      <td>8,028,160</td>\n",
       "      <td>1,239,040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-15b</th>\n",
       "      <td>0</td>\n",
       "      <td>15,655,899,136</td>\n",
       "      <td>122,880</td>\n",
       "      <td>12,124,160</td>\n",
       "      <td>1,249,280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-16B_P</th>\n",
       "      <td>0</td>\n",
       "      <td>16,032,155,648</td>\n",
       "      <td>122,880</td>\n",
       "      <td>13,369,344</td>\n",
       "      <td>1,462,272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pre-trained     fine-tuning prompt-tuning        lora  \\\n",
       "model                                                                      \n",
       "codegen-350M-multi           0     356,712,448        20,480   1,310,720   \n",
       "codegen2-1B_P                0   1,015,306,240        40,960   2,097,152   \n",
       "starcoder2-3b                0   3,030,371,328        61,440   4,546,560   \n",
       "codegen2-3_7B_P              0   3,641,174,016        81,920   4,194,304   \n",
       "CodeLlama-7b-hf              0   6,738,546,688        81,920   8,388,608   \n",
       "codegen2-7B_P                0   6,862,858,240        81,920   8,388,608   \n",
       "starcoder2-7b                0   7,173,923,840        92,160   7,340,032   \n",
       "starcoderbase                0  15,517,456,384       122,880   8,028,160   \n",
       "starcoder2-15b               0  15,655,899,136       122,880  12,124,160   \n",
       "codegen2-16B_P               0  16,032,155,648       122,880  13,369,344   \n",
       "\n",
       "                          ia3  \n",
       "model                          \n",
       "codegen-350M-multi    143,360  \n",
       "codegen2-1B_P         229,376  \n",
       "starcoder2-3b         468,480  \n",
       "codegen2-3_7B_P       458,752  \n",
       "CodeLlama-7b-hf       614,400  \n",
       "codegen2-7B_P         917,504  \n",
       "starcoder2-7b         753,664  \n",
       "starcoderbase       1,239,040  \n",
       "starcoder2-15b      1,249,280  \n",
       "codegen2-16B_P      1,462,272  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainable_params =  pd.read_csv(DATA_DIR / 'params_data.csv', index_col=0)\n",
    "model_trainable_params.index = model_trainable_params.index.str.split('/').str[1]\n",
    "model_trainable_params = model_trainable_params.loc[models.keys()]\n",
    "model_trainable_params[\"pre-trained\"] = '0'\n",
    "model_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c89b0da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">methods2test_runnable</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">humaneval-x</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">valid_syntax</th>\n",
       "      <th colspan=\"5\" halign=\"left\">scores</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"5\" halign=\"left\">coverage_instruction</th>\n",
       "      <th colspan=\"5\" halign=\"left\">coverage_branch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>lora</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>...</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>codegen-350M-multi</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-1B_P</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-3b</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-3_7B_P</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-7b-hf</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-7B_P</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-7b</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoderbase</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-15b</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-16B_P</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   methods2test_runnable                                  \\\n",
       "                            valid_syntax                                   \n",
       "                                    lora   ia3 prompt-tuning pre-trained   \n",
       "model                                                                      \n",
       "codegen-350M-multi                  0.96  0.95          0.96        0.96   \n",
       "codegen2-1B_P                       0.38  0.02          0.70        0.00   \n",
       "starcoder2-3b                       0.98  0.94          0.93        0.93   \n",
       "codegen2-3_7B_P                     0.42  0.00          0.22        0.00   \n",
       "CodeLlama-7b-hf                     0.98  0.98          0.97        0.98   \n",
       "codegen2-7B_P                       0.99  0.98          0.98        0.99   \n",
       "starcoder2-7b                       0.97  0.95          0.93        0.92   \n",
       "starcoderbase                       0.92  0.92          0.91        0.92   \n",
       "starcoder2-15b                      0.98  0.93          0.98        0.93   \n",
       "codegen2-16B_P                      0.99  0.98          0.98        0.98   \n",
       "\n",
       "                                                                            \\\n",
       "                                    scores                                   \n",
       "                   fine-tuning pre-trained fine-tuning   ia3 prompt-tuning   \n",
       "model                                                                        \n",
       "codegen-350M-multi        0.98        0.24        0.30  0.24          0.23   \n",
       "codegen2-1B_P             0.75        0.00        0.15  0.26          0.26   \n",
       "starcoder2-3b             0.96        0.17        0.30  0.29          0.17   \n",
       "codegen2-3_7B_P           0.41        0.00        0.11  0.00          0.26   \n",
       "CodeLlama-7b-hf           0.98        0.31        0.32  0.31          0.30   \n",
       "codegen2-7B_P             0.98        0.28        0.31  0.29          0.28   \n",
       "starcoder2-7b             0.97        0.17        0.31  0.32          0.33   \n",
       "starcoderbase             0.97        0.17        0.34  0.17          0.19   \n",
       "starcoder2-15b            0.98        0.20        0.34  0.20          0.33   \n",
       "codegen2-16B_P            0.99        0.30        0.33  0.30          0.30   \n",
       "\n",
       "                          ...          humaneval-x                    \\\n",
       "                          ... coverage_instruction                     \n",
       "                    lora  ...          pre-trained fine-tuning   ia3   \n",
       "model                     ...                                          \n",
       "codegen-350M-multi  0.26  ...                 0.97        1.00  0.99   \n",
       "codegen2-1B_P       0.04  ...                 0.00        0.00  0.00   \n",
       "starcoder2-3b       0.31  ...                 0.99        0.99  0.99   \n",
       "codegen2-3_7B_P     0.11  ...                 0.00        0.00  0.00   \n",
       "CodeLlama-7b-hf     0.34  ...                 0.98        0.99  0.99   \n",
       "codegen2-7B_P       0.31  ...                 0.98        1.00  0.98   \n",
       "starcoder2-7b       0.31  ...                 0.99        0.99  0.99   \n",
       "starcoderbase       0.17  ...                 0.99        0.99  0.99   \n",
       "starcoder2-15b      0.34  ...                 0.99        1.00  0.99   \n",
       "codegen2-16B_P      0.33  ...                 0.99        1.00  0.99   \n",
       "\n",
       "                                                                          \\\n",
       "                                       coverage_branch                     \n",
       "                   prompt-tuning  lora     pre-trained fine-tuning   ia3   \n",
       "model                                                                      \n",
       "codegen-350M-multi          0.98  0.97            0.90        0.83  0.94   \n",
       "codegen2-1B_P               0.00  0.00            0.00        0.00  0.00   \n",
       "starcoder2-3b               1.00  1.00            0.85        0.85  0.83   \n",
       "codegen2-3_7B_P             0.00  0.00            0.00        0.00  0.00   \n",
       "CodeLlama-7b-hf             0.98  1.00            0.84        0.86  0.85   \n",
       "codegen2-7B_P               0.99  0.99            0.81        0.70  0.80   \n",
       "starcoder2-7b               0.99  0.99            0.83        0.90  0.88   \n",
       "starcoderbase               0.99  1.00            0.81        0.75  0.81   \n",
       "starcoder2-15b              1.00  0.99            0.88        0.81  0.88   \n",
       "codegen2-16B_P              0.97  0.99            0.81        0.78  0.81   \n",
       "\n",
       "                                        \n",
       "                                        \n",
       "                   prompt-tuning  lora  \n",
       "model                                   \n",
       "codegen-350M-multi          0.92  0.89  \n",
       "codegen2-1B_P               0.00  0.00  \n",
       "starcoder2-3b               0.76  0.57  \n",
       "codegen2-3_7B_P             0.00  0.00  \n",
       "CodeLlama-7b-hf             0.82  0.72  \n",
       "codegen2-7B_P               0.69  0.75  \n",
       "starcoder2-7b               0.86  0.87  \n",
       "starcoderbase               0.68  0.74  \n",
       "starcoder2-15b              0.83  0.83  \n",
       "codegen2-16B_P              0.71  0.82  \n",
       "\n",
       "[10 rows x 50 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    dataset_dir = DATA_DIR / dataset\n",
    "    \n",
    "    coverage_branch = pd.read_csv(dataset_dir / 'coverage_branch.csv', index_col=0)\n",
    "    coverage_branch.index = coverage_branch.index.str.split('/').str[1]\n",
    "    coverage_branch = coverage_branch.loc[models.keys()]\n",
    "    \n",
    "    coverage_instruction = pd.read_csv(dataset_dir / 'coverage_instruction.csv', index_col=0)\n",
    "    coverage_instruction.index = coverage_instruction.index.str.split('/').str[1]\n",
    "    coverage_instruction = coverage_instruction.loc[models.keys()]\n",
    "    \n",
    "    coverage_runnable = pd.read_csv(dataset_dir / 'coverage_runnable.csv', index_col=0)\n",
    "    coverage_runnable.index = coverage_runnable.index.str.split('/').str[1]\n",
    "    coverage_runnable = coverage_runnable.loc[models.keys()]\n",
    "    \n",
    "    \n",
    "    scores = pd.read_csv(dataset_dir / 'scores.csv', index_col=0)\n",
    "    scores.index = scores.index.str.split('/').str[1]\n",
    "    scores = scores.loc[models.keys()]\n",
    "    \n",
    "    valid_syntax = pd.read_csv(dataset_dir / 'valid_syntax.csv', index_col=0)\n",
    "    valid_syntax.index = valid_syntax.index.str.split('/').str[1]\n",
    "    valid_syntax = valid_syntax.loc[models.keys()]\n",
    "    \n",
    "    data[dataset] = pd.concat([valid_syntax, scores, coverage_runnable, coverage_instruction, coverage_branch], axis=1, keys=columns)\n",
    "    \n",
    "\n",
    "#concat all datasets\n",
    "data = pd.concat(data, axis=1)\n",
    "data = data.astype(float).round(2)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08418048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'codegen-350M-multi': [('methods2test_runnable',\n",
       "   'valid_syntax',\n",
       "   'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3')],\n",
       " 'codegen2-1B_P': [('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'ia3'),\n",
       "  ('methods2test_runnable', 'scores', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'starcoder2-3b': [('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'scores', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning')],\n",
       " 'codegen2-3_7B_P': [('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'scores', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'CodeLlama-7b-hf': [('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'ia3'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning')],\n",
       " 'codegen2-7B_P': [('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'scores', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained')],\n",
       " 'starcoder2-7b': [('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning')],\n",
       " 'starcoderbase': [('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3')],\n",
       " 'starcoder2-15b': [('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3')],\n",
       " 'codegen2-16B_P': [('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'pre-trained'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_method_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for column in columns:\n",
    "        for model in models.keys():\n",
    "            model_data = data[dataset, column].loc[model]\n",
    "            # Find max value and what method it corresponds to\n",
    "            max_value = model_data.max()\n",
    "            max_indices = model_data[model_data == max_value].index.tolist()\n",
    "            # If all values are the same, max_indices will be empty\n",
    "            #if set(max_indices) == set(model_data.index.tolist()):\n",
    "            #    print(f\"All values are the same for {dataset}, {column}, {model}.\")\n",
    "            #    continue\n",
    "            \n",
    "            for method_index in max_indices:\n",
    "                best_method_data.setdefault(model, [])\n",
    "                best_method_data[model].append((dataset, column, method_index))\n",
    "                \n",
    "best_method_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a631b8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'codegen-350M-multi': [('methods2test_runnable', 'valid_syntax', 'ia3'),\n",
       "  ('methods2test_runnable', 'scores', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'CodeLlama-7b-hf': [('methods2test_runnable',\n",
       "   'valid_syntax',\n",
       "   'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'codegen2-7B_P': [('methods2test_runnable', 'valid_syntax', 'ia3'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'starcoderbase': [('methods2test_runnable', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'starcoder2-3b': [('methods2test_runnable',\n",
       "   'coverage_runnable',\n",
       "   'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'starcoder2-7b': [('methods2test_runnable',\n",
       "   'coverage_runnable',\n",
       "   'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora')],\n",
       " 'starcoder2-15b': [('methods2test_runnable',\n",
       "   'coverage_runnable',\n",
       "   'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'codegen2-16B_P': [('methods2test_runnable',\n",
       "   'coverage_runnable',\n",
       "   'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning')]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decreased_performance_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for column in columns:\n",
    "        for model in models.keys():\n",
    "            model_data = data[dataset, column].loc[model]\n",
    "            # Find max value and what method it corresponds to\n",
    "            baseline_value = model_data.loc['pre-trained']\n",
    "            \n",
    "            decreased_indices = model_data[model_data < baseline_value].index.tolist()\n",
    "            \n",
    "            for method_index in decreased_indices:\n",
    "                decreased_performance_data.setdefault(model, [])\n",
    "                decreased_performance_data[model].append((dataset, column, method_index))\n",
    "\n",
    "decreased_performance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55bf7d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'codegen2-1B_P': [('methods2test_runnable', 'valid_syntax', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'scores', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'scores', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'lora'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'ia3'),\n",
       "  ('methods2test_runnable', 'scores', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'scores', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning')],\n",
       " 'codegen2-3_7B_P': [('methods2test_runnable', 'valid_syntax', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'scores', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'scores', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'lora'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'ia3'),\n",
       "  ('methods2test_runnable', 'scores', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'ia3'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'scores', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning')]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_syntactical_performance_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for method in methods.keys():\n",
    "        for model in models.keys():\n",
    "            valid_syntax = data[dataset, \"valid_syntax\", method].loc[model]\n",
    "            if valid_syntax < 0.5:\n",
    "                for column in columns:  # Skip 'valid_syntax'\n",
    "                    bad_syntactical_performance_data.setdefault(model, [])\n",
    "                    bad_syntactical_performance_data[model].append((dataset, column, method))\n",
    "                    \n",
    "\n",
    "bad_syntactical_performance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ae2776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_latex(text):\n",
    "    \"\"\"Escape LaTeX special characters.\"\"\"\n",
    "    return text.replace('_', '\\\\_').replace('%', '\\\\%').replace('&', '\\\\&').replace('$', '\\\\$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d702838",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "\n",
    "table.append(\"\\\\begin{table*}[htbp]\")\n",
    "table.append(\"\\\\begin{threeparttable}\")\n",
    "table.append(\"    \\\\newcolumntype{Y}{>{\\\\centering\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\newcolumntype{R}{>{\\\\raggedright\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\newcolumntype{L}{>{\\\\raggedleft\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\centering\")\n",
    "table.append(\"    \\\\footnotesize\")\n",
    "table.append(\"    \\\\caption{Comparison of syntactical validity and CodeBLEU scores from experiments using different tuning methods across various models on testing split of \\\\textsc{Methods2Test\\\\textsubscript{runnable}} and \\textsc{HumanEval-X\\\\textsubscript{java}} datasets. }\\\\label{tab:eval-summary}\")\n",
    "\n",
    "row = \"    \\\\begin{tabularx}{\\\\textwidth}{lr\"\n",
    "for i, dataset in enumerate(datasets, start=1):\n",
    "    row += \"L\" * 5  # 5 columns for each dataset\n",
    "    if i < len(datasets):\n",
    "        row += \"!{\\\\color{white}\\\\ }\"  # Add a space for the vertical line\n",
    "row += \"}\"\n",
    "table.append(row)\n",
    "#table.append(\"    \\\\begin{tabularx}{\\\\textwidth}{lrLLLLL!{\\\\color{white}\\\\ }LLLLL}\")\n",
    "\n",
    "\n",
    "table.append(\"        \\\\toprule\")\n",
    "\n",
    "row = \"        \\\\multirow{2}{*}{\\\\textbf{Method}} & \\\\multirow{3}{*}{\\\\parbox[t]{1cm}{\\\\centering \\\\textbf{Trainable\\\\\\\\params}}}\"\n",
    "for dataset in datasets:\n",
    "    row += \" & \\\\multicolumn{5}{c}{\\\\textbf{\" + datasets[dataset] + \"}}\"\n",
    "row += \"\\\\\\\\\"\n",
    "table.append(row)\n",
    "\n",
    "#\\cmidrule(lr){3-7}\\cmidrule(lr){8-12}\n",
    "row = \"        \"\n",
    "for i, dataset in enumerate(datasets):\n",
    "    index = 3 + (i * 5)\n",
    "    row += \"\\\\cmidrule(lr){\" + f\"{index}-{index + 4}\" + \"}\"\n",
    "table.append(row)\n",
    "\n",
    "row  = \"        &\"\n",
    "for dataset in datasets:\n",
    "    row += \" & \\\\rotatebox[origin=l]{90}{Valid syntax} & \\\\rotatebox[origin=l]{90}{CodeBLEU} & \\\\rotatebox[origin=l]{90}{pass@1} & \\\\rotatebox[origin=l]{90}{Instr Cov} & \\\\rotatebox[origin=l]{90}{Branch Cov}\"\n",
    "row += \"\\\\\\\\\"\n",
    "table.append(row)\n",
    "\n",
    "table.append(\"        \\\\hline\")\n",
    "\n",
    "\n",
    "for model in models.keys():\n",
    "    table.append(\"        \\\\multicolumn{\" + str(2+5*len(datasets)) + \"}{l}{\\\\cellcolor{gray!10}{\\\\textbf{\" + models[model] + \"}}} \\\\bigstrut \\\\\\\\*\")\n",
    "    for method in methods:\n",
    "        col = []\n",
    "        for dataset in datasets:\n",
    "            for column in columns:\n",
    "                try:\n",
    "                    value = data[dataset, column, method].loc[model]\n",
    "                except KeyError:\n",
    "                    value = \"N/A\"\n",
    "            \n",
    "                if (dataset, column, method) in decreased_performance_data.get(model, []):\n",
    "                    value = f\"({value})\"\n",
    "                    \n",
    "                if (dataset, column, method) in best_method_data.get(model, []):\n",
    "                    value = f\"\\\\textbf{{{value}}}\"\n",
    "                    \n",
    "                if (dataset, column, method) in bad_syntactical_performance_data.get(model, []):\n",
    "                    value = f\"\\\\cellcolor{{red!10}}{{{value}}}\"\n",
    "                    \n",
    "                col.append(f\"{value}\")\n",
    "        row = \" & \".join(col)\n",
    "        params = model_trainable_params[method].loc[model]\n",
    "        row = \"        \" + methods[method] + \" & \" + params + \" & \" + row + \" \\\\\\\\\"\n",
    "        table.append(row)\n",
    "    table.append(\"\")\n",
    "\n",
    "table.append(\"       \\\\bottomrule\")\n",
    "table.append(\"    \\\\end{tabularx}\")\n",
    "table.append(\"    \\\\begin{tablenotes}[flushleft]\\\\small\")\n",
    "table.append(\"      \\\\item \\\\textbf{Bold}: best-performing training method per model. (Parentheses): decreased performance compared to baseline. \\\\colorbox{red!10}{Red}: $<$ 50\\\\% syntactical valid samples. \\\\underline{Underline}: Other notable results (see in \\\\Cref{sec:syntax}).\")\n",
    "table.append(\"    \\\\end{tablenotes}\")\n",
    "table.append(\"\\\\end{threeparttable}\")\n",
    "table.append(\"\\\\end{table*}\")\n",
    "\n",
    "#print(\"\\n\".join(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "720e0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"\\n\".join(table)\n",
    "table_path = Path.cwd().parent / 'tables' / 'eval_summary.tex'\n",
    "table_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(table_path, 'w') as f:\n",
    "    f.write(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
