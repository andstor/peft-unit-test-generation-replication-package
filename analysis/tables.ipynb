{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a8baac",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae804c65",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c2f66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc63bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'codegen-350M-multi': 'CodeGen-350M-multi',\n",
    "    'codegen2-1B_P': 'CodeGen2-1B',\n",
    "    'starcoder2-3b': 'StarCoder2-3B',\n",
    "    'codegen2-3_7B_P': 'CodeGen2-3.7B',\n",
    "    'CodeLlama-7b-hf': 'CodeLlama-7B',\n",
    "    'codegen2-7B_P': 'CodeGen2-7B',\n",
    "    'starcoder2-7b': 'StarCoder2-7B',\n",
    "    'starcoderbase': 'StarCoderBase',\n",
    "    'starcoder2-15b': 'StarCoder2-15B',\n",
    "    'codegen2-16B_P': 'CodeGen2-16B',\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    'methods2test_runnable': '\\\\textsc{Methods2Test\\\\textsubscript{runnable}}',\n",
    "    'humaneval-x': '\\\\textsc{HumanEval-X\\\\textsubscript{java}}',\n",
    "}\n",
    "\n",
    "columns = {\"methods2test_runnable\": ['valid_syntax', 'scores', 'passing_rate', 'coverage_instruction', 'coverage_branch'],\n",
    "           \"humaneval-x\": ['valid_syntax', 'scores', 'passing_rate']}\n",
    "\n",
    "methods = {\n",
    "    'pre-trained': 'None',\n",
    "    'fine-tuning': 'Fine-tuning',\n",
    "    'lora': 'LoRA',\n",
    "    'ia3': '(IA)\\\\textsuperscript{3}',\n",
    "    'prompt-tuning': 'Prompt tuning',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444cf47f",
   "metadata": {},
   "source": [
    "## Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588eedf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>ia3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>codegen-350M-multi</th>\n",
       "      <td>0</td>\n",
       "      <td>356,712,448</td>\n",
       "      <td>20,480</td>\n",
       "      <td>1,310,720</td>\n",
       "      <td>143,360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-1B_P</th>\n",
       "      <td>0</td>\n",
       "      <td>1,015,306,240</td>\n",
       "      <td>40,960</td>\n",
       "      <td>2,097,152</td>\n",
       "      <td>229,376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-3b</th>\n",
       "      <td>0</td>\n",
       "      <td>3,030,371,328</td>\n",
       "      <td>61,440</td>\n",
       "      <td>4,546,560</td>\n",
       "      <td>468,480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-3_7B_P</th>\n",
       "      <td>0</td>\n",
       "      <td>3,641,174,016</td>\n",
       "      <td>81,920</td>\n",
       "      <td>4,194,304</td>\n",
       "      <td>458,752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-7b-hf</th>\n",
       "      <td>0</td>\n",
       "      <td>6,738,546,688</td>\n",
       "      <td>81,920</td>\n",
       "      <td>8,388,608</td>\n",
       "      <td>614,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-7B_P</th>\n",
       "      <td>0</td>\n",
       "      <td>6,862,858,240</td>\n",
       "      <td>81,920</td>\n",
       "      <td>8,388,608</td>\n",
       "      <td>917,504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-7b</th>\n",
       "      <td>0</td>\n",
       "      <td>7,173,923,840</td>\n",
       "      <td>92,160</td>\n",
       "      <td>7,340,032</td>\n",
       "      <td>753,664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoderbase</th>\n",
       "      <td>0</td>\n",
       "      <td>15,517,456,384</td>\n",
       "      <td>122,880</td>\n",
       "      <td>8,028,160</td>\n",
       "      <td>1,239,040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-15b</th>\n",
       "      <td>0</td>\n",
       "      <td>15,655,899,136</td>\n",
       "      <td>122,880</td>\n",
       "      <td>12,124,160</td>\n",
       "      <td>1,249,280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-16B_P</th>\n",
       "      <td>0</td>\n",
       "      <td>16,032,155,648</td>\n",
       "      <td>122,880</td>\n",
       "      <td>13,369,344</td>\n",
       "      <td>1,462,272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pre-trained     fine-tuning prompt-tuning        lora  \\\n",
       "model                                                                      \n",
       "codegen-350M-multi           0     356,712,448        20,480   1,310,720   \n",
       "codegen2-1B_P                0   1,015,306,240        40,960   2,097,152   \n",
       "starcoder2-3b                0   3,030,371,328        61,440   4,546,560   \n",
       "codegen2-3_7B_P              0   3,641,174,016        81,920   4,194,304   \n",
       "CodeLlama-7b-hf              0   6,738,546,688        81,920   8,388,608   \n",
       "codegen2-7B_P                0   6,862,858,240        81,920   8,388,608   \n",
       "starcoder2-7b                0   7,173,923,840        92,160   7,340,032   \n",
       "starcoderbase                0  15,517,456,384       122,880   8,028,160   \n",
       "starcoder2-15b               0  15,655,899,136       122,880  12,124,160   \n",
       "codegen2-16B_P               0  16,032,155,648       122,880  13,369,344   \n",
       "\n",
       "                          ia3  \n",
       "model                          \n",
       "codegen-350M-multi    143,360  \n",
       "codegen2-1B_P         229,376  \n",
       "starcoder2-3b         468,480  \n",
       "codegen2-3_7B_P       458,752  \n",
       "CodeLlama-7b-hf       614,400  \n",
       "codegen2-7B_P         917,504  \n",
       "starcoder2-7b         753,664  \n",
       "starcoderbase       1,239,040  \n",
       "starcoder2-15b      1,249,280  \n",
       "codegen2-16B_P      1,462,272  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainable_params =  pd.read_csv(DATA_DIR / 'params_data.csv', index_col=0)\n",
    "model_trainable_params.index = model_trainable_params.index.str.split('/').str[1]\n",
    "model_trainable_params = model_trainable_params.loc[models.keys()]\n",
    "model_trainable_params[\"pre-trained\"] = '0'\n",
    "model_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56418cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">methods2test_runnable</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">humaneval-x</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">valid_syntax</th>\n",
       "      <th colspan=\"5\" halign=\"left\">scores</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"5\" halign=\"left\">scores</th>\n",
       "      <th colspan=\"5\" halign=\"left\">passing_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>lora</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>...</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>codegen-350M-multi</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-1B_P</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-3b</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-3_7B_P</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-7b-hf</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-7B_P</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-7b</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoderbase</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-15b</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-16B_P</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   methods2test_runnable                                  \\\n",
       "                            valid_syntax                                   \n",
       "                                    lora   ia3 prompt-tuning pre-trained   \n",
       "model                                                                      \n",
       "codegen-350M-multi                  0.96  0.95          0.96        0.96   \n",
       "codegen2-1B_P                       0.38  0.02          0.70        0.00   \n",
       "starcoder2-3b                       0.98  0.94          0.93        0.93   \n",
       "codegen2-3_7B_P                     0.42  0.00          0.22        0.00   \n",
       "CodeLlama-7b-hf                     0.98  0.98          0.97        0.98   \n",
       "codegen2-7B_P                       0.99  0.98          0.98        0.99   \n",
       "starcoder2-7b                       0.97  0.95          0.93        0.92   \n",
       "starcoderbase                       0.92  0.92          0.91        0.92   \n",
       "starcoder2-15b                      0.98  0.93          0.98        0.93   \n",
       "codegen2-16B_P                      0.99  0.98          0.98        0.98   \n",
       "\n",
       "                                                                            \\\n",
       "                                    scores                                   \n",
       "                   fine-tuning pre-trained fine-tuning   ia3 prompt-tuning   \n",
       "model                                                                        \n",
       "codegen-350M-multi        0.98        0.24        0.30  0.24          0.23   \n",
       "codegen2-1B_P             0.75        0.00        0.15  0.26          0.26   \n",
       "starcoder2-3b             0.96        0.17        0.30  0.29          0.17   \n",
       "codegen2-3_7B_P           0.41        0.00        0.11  0.00          0.26   \n",
       "CodeLlama-7b-hf           0.98        0.31        0.32  0.31          0.30   \n",
       "codegen2-7B_P             0.98        0.28        0.31  0.29          0.28   \n",
       "starcoder2-7b             0.97        0.17        0.31  0.32          0.33   \n",
       "starcoderbase             0.97        0.17        0.34  0.17          0.19   \n",
       "starcoder2-15b            0.98        0.20        0.34  0.20          0.33   \n",
       "codegen2-16B_P            0.99        0.30        0.33  0.30          0.30   \n",
       "\n",
       "                          ... humaneval-x                                  \\\n",
       "                          ...      scores                                   \n",
       "                    lora  ... pre-trained fine-tuning   ia3 prompt-tuning   \n",
       "model                     ...                                               \n",
       "codegen-350M-multi  0.26  ...        0.36        0.33  0.36          0.33   \n",
       "codegen2-1B_P       0.04  ...        0.00        0.04  0.00          0.25   \n",
       "starcoder2-3b       0.31  ...        0.43        0.50  0.42          0.43   \n",
       "codegen2-3_7B_P     0.11  ...        0.00        0.26  0.00          0.00   \n",
       "CodeLlama-7b-hf     0.34  ...        0.49        0.50  0.48          0.46   \n",
       "codegen2-7B_P       0.31  ...        0.47        0.44  0.49          0.44   \n",
       "starcoder2-7b       0.31  ...        0.40        0.44  0.42          0.51   \n",
       "starcoderbase       0.17  ...        0.43        0.48  0.43          0.23   \n",
       "starcoder2-15b      0.34  ...        0.37        0.49  0.37          0.54   \n",
       "codegen2-16B_P      0.33  ...        0.48        0.38  0.48          0.47   \n",
       "\n",
       "                                                                             \n",
       "                         passing_rate                                        \n",
       "                    lora  pre-trained fine-tuning   ia3 prompt-tuning  lora  \n",
       "model                                                                        \n",
       "codegen-350M-multi  0.39         0.07        0.04  0.07          0.05  0.04  \n",
       "codegen2-1B_P       0.01         0.00        0.00  0.00          0.00  0.00  \n",
       "starcoder2-3b       0.42         0.34        0.31  0.36          0.27  0.24  \n",
       "codegen2-3_7B_P     0.14         0.00        0.00  0.00          0.00  0.00  \n",
       "CodeLlama-7b-hf     0.43         0.33        0.34  0.32          0.28  0.31  \n",
       "codegen2-7B_P       0.47         0.23        0.13  0.21          0.18  0.20  \n",
       "starcoder2-7b       0.52         0.37        0.36  0.34          0.37  0.35  \n",
       "starcoderbase       0.39         0.30        0.33  0.30          0.16  0.30  \n",
       "starcoder2-15b      0.46         0.41        0.37  0.41          0.45  0.39  \n",
       "codegen2-16B_P      0.47         0.20        0.12  0.20          0.21  0.22  \n",
       "\n",
       "[10 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    dataset_dir = DATA_DIR / dataset\n",
    "    dataset_data = {}\n",
    "    dataset_columns = []\n",
    "\n",
    "    for name in columns[dataset]:\n",
    "        file_path = dataset_dir / f\"{name}.csv\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, index_col=0)\n",
    "            df.index = df.index.str.split('/').str[1]\n",
    "            df = df.loc[models.keys()]\n",
    "            dataset_data[name] = df\n",
    "            dataset_columns.append(name)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Missing file: {file_path} — skipping.\")\n",
    "\n",
    "    if dataset_data:\n",
    "        data[dataset] = pd.concat(\n",
    "            [dataset_data[col] for col in dataset_columns],\n",
    "            axis=1,\n",
    "            keys=dataset_columns\n",
    "        )\n",
    "\n",
    "# Combine all datasets (outer concat across dataset names)\n",
    "if data:\n",
    "    data = pd.concat(data, axis=1)\n",
    "    data = data.astype(float).round(2)\n",
    "else:\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08418048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/v12j1gk96gz44lcngd9z6ddh0000gn/T/ipykernel_84207/3602500784.py:6: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  model_data = data[dataset, column].loc[model]\n"
     ]
    }
   ],
   "source": [
    "best_method_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for column in columns[dataset]:\n",
    "        for model in models.keys():\n",
    "            model_data = data[dataset, column].loc[model]\n",
    "            # Find max value and what method it corresponds to\n",
    "            max_value = model_data.max()\n",
    "            max_indices = model_data[model_data == max_value].index.tolist()\n",
    "            # If all values are the same, max_indices will be empty\n",
    "            #if set(max_indices) == set(model_data.index.tolist()):\n",
    "            #    print(f\"All values are the same for {dataset}, {column}, {model}.\")\n",
    "            #    continue\n",
    "            \n",
    "            for method_index in max_indices:\n",
    "                best_method_data.setdefault(model, [])\n",
    "                best_method_data[model].append((dataset, column, method_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a631b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/v12j1gk96gz44lcngd9z6ddh0000gn/T/ipykernel_84207/2546448888.py:6: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  model_data = data[dataset, column].loc[model]\n"
     ]
    }
   ],
   "source": [
    "decreased_performance_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for column in columns[dataset]:\n",
    "        for model in models.keys():\n",
    "            model_data = data[dataset, column].loc[model]\n",
    "            # Find max value and what method it corresponds to\n",
    "            \n",
    "            baseline_value = model_data.loc['pre-trained']\n",
    "            \n",
    "            decreased_indices = model_data[model_data < baseline_value].index.tolist()\n",
    "            \n",
    "            for method_index in decreased_indices:\n",
    "                decreased_performance_data.setdefault(model, [])\n",
    "                decreased_performance_data[model].append((dataset, column, method_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55bf7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_syntactical_performance_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for method in methods.keys():\n",
    "        for model in models.keys():\n",
    "            valid_syntax = data[dataset, \"valid_syntax\", method].loc[model]\n",
    "            if valid_syntax < 0.5:\n",
    "                for column in columns[dataset]:  # Skip 'valid_syntax'\n",
    "                    bad_syntactical_performance_data.setdefault(model, [])\n",
    "                    bad_syntactical_performance_data[model].append((dataset, column, method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae2776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_latex(text):\n",
    "    \"\"\"Escape LaTeX special characters.\"\"\"\n",
    "    return text.replace('_', '\\\\_').replace('%', '\\\\%').replace('&', '\\\\&').replace('$', '\\\\$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61900ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_letter(n):\n",
    "    return chr(ord('a') + n - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d702838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "methods2test_runnable\n",
      "Adding dataset methods2test_runnable with 5 columns\n",
      "humaneval-x\n"
     ]
    }
   ],
   "source": [
    "table = []\n",
    "\n",
    "table.append(\"\\\\begin{table*}[htbp]\")\n",
    "table.append(\"\\\\begin{threeparttable}\")\n",
    "table.append(\"    \\\\newcolumntype{Y}{>{\\\\centering\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\newcolumntype{R}{>{\\\\raggedright\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\newcolumntype{L}{>{\\\\raggedleft\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\centering\")\n",
    "table.append(\"    \\\\footnotesize\")\n",
    "table.append(\"    \\\\caption{Evaluation metrics experiment results using different tuning methods across various models.}\\\\label{tab:eval-summary}\")\n",
    "\n",
    "row = \"    \\\\begin{tabularx}{\\\\textwidth}{lr!{\\\\color{white}\\\\hspace{.5em}}\"\n",
    "for i, dataset in enumerate(datasets, start=1):\n",
    "    \n",
    "    row += \"Y\" * len(columns[dataset])  # 5 columns for each dataset\n",
    "    if i < len(datasets):\n",
    "        row += \"!{\\\\color{white}\\\\hspace{1em}}\"\n",
    "row += \"}\"\n",
    "table.append(row)\n",
    "#table.append(\"    \\\\begin{tabularx}{\\\\textwidth}{lrLLLLL!{\\\\color{white}\\\\ }LLLLL}\")\n",
    "#\\multicolumn{2}{c}{\\normalsize\\textbf{(a)}} & \\multicolumn{5}{c}{\\normalsize\\textbf{(b)}} & \\multicolumn{3}{c}{\\normalsize\\textbf{(c)}}\\\\[.5em]\n",
    "\n",
    "row = \"        \\\\multicolumn{2}{c}{\\\\normalsize\\\\textbf{(a)}} & \"\n",
    "for i, dataset in enumerate(datasets, start=2):\n",
    "    print(dataset)\n",
    "    row += \"\\\\multicolumn{\" + str(len(columns[dataset])) + \"}{c}{\\\\normalsize\\\\textbf{(\" + str(int_to_letter(i)) + \")}}\"\n",
    "    if i < len(datasets) + 1:\n",
    "        print(f\"Adding dataset {dataset} with {len(columns[dataset])} columns\")\n",
    "        row += \" & \"\n",
    "row += \"\\\\\\\\[.5em]\"\n",
    "table.append(row)\n",
    "\n",
    "table.append(\"        \\\\cmidrule(lr){1-2}\\\\cmidrule(lr){3-7}\\\\cmidrule(lr){8-10}\")\n",
    "\n",
    "row = \"        \\\\multirow{2}{*}{\\\\textbf{Method}} & \\\\multirow{2}{*}{\\\\parbox[t]{1cm}{\\\\centering \\\\textbf{Trainable\\\\\\\\params}}}\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    row += \" & \\\\multicolumn{\" + str(len(columns[dataset])) + \"}{c}{\\\\textbf{\" + escape_latex(datasets[dataset]) + \"}}\"\n",
    "#row += \" & \\\\multicolumn{5}{c}{\\\\textbf{\" + datasets[dataset] + \"}}\"\n",
    "#row += \" & \\\\multicolumn{3}{c}{\\\\textbf{\" + datasets[dataset] + \"}}\"\n",
    "row += \"\\\\\\\\\"\n",
    "table.append(row)\n",
    "\n",
    "#\\cmidrule(lr){3-7}\\cmidrule(lr){8-12}\n",
    "row = \"        \"\n",
    "index = 3\n",
    "for i, dataset in enumerate(datasets):\n",
    "    row += \"\\\\cmidrule(lr){\" + f\"{index}-{index + len(columns[dataset])-1}\" + \"}\"\n",
    "    index += len(columns[dataset])\n",
    "table.append(row)\n",
    "\n",
    "row  = \"        &\"\n",
    "row += \" & \\\\rotatebox[origin=l]{90}{Valid syntax} & \\\\rotatebox[origin=l]{90}{CodeBLEU} & \\\\rotatebox[origin=l]{90}{pass@1} & \\\\rotatebox[origin=l]{90}{Instr. Cov.} & \\\\rotatebox[origin=l]{90}{Branch Cov.}\"\n",
    "row += \" & \\\\rotatebox[origin=l]{90}{Valid syntax} & \\\\rotatebox[origin=l]{90}{CodeBLEU} & \\\\rotatebox[origin=l]{90}{pass@1}\"\n",
    "row += \"\\\\\\\\\"\n",
    "table.append(row)\n",
    "\n",
    "table.append(\"        \\\\hline\")\n",
    "\n",
    "\n",
    "for model in models.keys():\n",
    "    table.append(\"        \\\\multicolumn{\" + str(2+5+3) + \"}{l}{\\\\cellcolor{gray!10}{\\\\textbf{\" + models[model] + \"}}} \\\\bigstrut \\\\\\\\*\")\n",
    "    for method in methods:\n",
    "        col = []\n",
    "        for dataset in datasets:\n",
    "            for column in columns[dataset]:\n",
    "                try:\n",
    "                    value = data[dataset, column, method].loc[model]\n",
    "                except KeyError:\n",
    "                    value = \"N/A\"\n",
    "            \n",
    "                if (dataset, column, method) in decreased_performance_data.get(model, []):\n",
    "                    value = f\"({value})\"\n",
    "                    \n",
    "                if (dataset, column, method) in best_method_data.get(model, []):\n",
    "                    value = f\"\\\\textbf{{{value}}}\"\n",
    "                    \n",
    "                if (dataset, column, method) in bad_syntactical_performance_data.get(model, []):\n",
    "                    value = f\"\\\\cellcolor{{red!10}}{{{value}}}\"\n",
    "                    \n",
    "                col.append(f\"{value}\")\n",
    "        row = \" & \".join(col)\n",
    "        params = model_trainable_params[method].loc[model]\n",
    "        row = \"        \" + methods[method] + \" & \" + params + \" & \" + row + \" \\\\\\\\\"\n",
    "        table.append(row)\n",
    "    table.append(\"\")\n",
    "\n",
    "table.append(\"       \\\\bottomrule\")\n",
    "table.append(\"    \\\\end{tabularx}\")\n",
    "table.append(\"    \\\\begin{tablenotes}[flushleft]\\\\small\")\n",
    "table.append(\"      \\\\item \\\\textbf{Bold}: best-performing training method per model. (Parentheses): decreased performance compared to baseline. \\\\colorbox{red!10}{Red}: $<$ 50\\\\% syntactical valid samples.\")\n",
    "table.append(\"    \\\\end{tablenotes}\")\n",
    "table.append(\"\\\\end{threeparttable}\")\n",
    "table.append(\"\\\\end{table*}\")\n",
    "\n",
    "#print(\"\\n\".join(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "720e0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"\\n\".join(table)\n",
    "table_path = Path.cwd().parent / 'tables' / 'eval_summary.tex'\n",
    "table_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(table_path, 'w') as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f9880",
   "metadata": {},
   "source": [
    "## Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7469d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"assert\", \"verify\", \"fail\"]\n",
    "\n",
    "def contains_keyword(text):\n",
    "    for keyword in keywords:\n",
    "        if keyword in text.lower():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aa1af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses = {\n",
    "    \"success\": \"Succ.\",\n",
    "    \"failed\": \"Failed\",\n",
    "    \"error\": \"Interrupt\",\n",
    "    \"compilation error\": \"CompErr\",\n",
    "    \"no_assertions\": \"NoAssert\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fab26764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table = []\n",
    "\n",
    "table.append(\"\\\\begin{table}[htbp]\")\n",
    "table.append(\"    \\\\newcolumntype{Y}{>{\\\\centering\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\centering\")\n",
    "table.append(\"    \\\\caption{Test execution statuses for StarCode2-7B.}\")\n",
    "table.append(\"    \\\\label{tab:test-statuses}\")\n",
    "table.append(\"    \\\\small\")\n",
    "table.append(\"    \\\\begin{tabularx}{\\\\columnwidth}{lYYYYY}\")\n",
    "table.append(\"    \\\\toprule\")\n",
    "table.append(\"    \\\\textbf{Method} & \\\\textbf{Succ.} & \\\\textbf{Failed} & \\\\textbf{Interrupt} & \\\\textbf{CompErr} & \\\\textbf{NoAssert} \\\\\\\\\")\n",
    "table.append(\"    \\\\midrule\")\n",
    "\n",
    "for method in methods:\n",
    "    col = []\n",
    "    \n",
    "    path = DATA_DIR / \"methods2test_runnable/coverage\" / method / \"bigcode/starcoder2-7b/jacoco.jsonl\"\n",
    "    df = pd.read_json(path, lines=True, dtype=False).set_index(\"id\")\n",
    "    df = df[df[\"status\"] != \"exception\"]\n",
    "    \n",
    "    gen_data_path = Path(str(path).replace(\"coverage\", \"fixed\").replace(\"jacoco.jsonl\", \"00001-of-00001.jsonl\"))\n",
    "    gen_df = pd.read_json(gen_data_path, lines=True, dtype=False).set_index(\"id\")\n",
    "    ids = gen_df[~gen_df[\"prediction\"].apply(contains_keyword)].index.tolist()\n",
    "    keys = df.index.intersection(ids)\n",
    "    no_assertions_df = df.loc[(df.index.isin(keys)) & (df[\"status\"] == \"success\")]\n",
    "    \n",
    "    values = df['status'].value_counts().sort_index()\n",
    "    values.loc['success'] = values.loc['success'] - no_assertions_df.shape[0]\n",
    "    values.loc['no_assertions'] = no_assertions_df.shape[0]\n",
    "\n",
    "    percentages = (values / values.sum())\n",
    "    \n",
    "    for status in statuses.keys():\n",
    "        value = str(int(percentages.loc[status].round(2) * 100)) + \"\\\\%\"\n",
    "        col.append(value)\n",
    "\n",
    "    \n",
    "    row = \" & \".join(col)\n",
    "    row = methods[method] + \" & \" + row\n",
    "    table.append(\"    \" + row + \" \\\\\\\\\")\n",
    "    \n",
    "table.append(\"    \\\\bottomrule\")\n",
    "table.append(\"    \\\\end{tabularx}\")\n",
    "table.append(\"\\\\end{table}\")\n",
    "\n",
    "#print(\"\\n\".join(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86befb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"\\n\".join(table)\n",
    "table_path = Path.cwd().parent / 'tables' / 'test_execution_example.tex'\n",
    "table_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(table_path, 'w') as f:\n",
    "    f.write(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
