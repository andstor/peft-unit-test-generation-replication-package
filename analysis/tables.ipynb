{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c2f66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc63bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'codegen-350M-multi': 'CodeGen-350M-multi',\n",
    "    'codegen2-1B_P': 'CodeGen2-1B',\n",
    "    'starcoder2-3b': 'StarCoder2-3B',\n",
    "    'codegen2-3_7B_P': 'CodeGen2-3.7B',\n",
    "    'CodeLlama-7b-hf': 'CodeLlama-7B',\n",
    "    'codegen2-7B_P': 'CodeGen2-7B',\n",
    "    'starcoder2-7b': 'StarCoder2-7B',\n",
    "    'starcoderbase': 'StarCoderBase',\n",
    "    'starcoder2-15b': 'StarCoder2-15B',\n",
    "    'codegen2-16B_P': 'CodeGen2-16B',\n",
    "}\n",
    "\n",
    "#datasets = ['methods2test_runnable', 'humaneval-x']\n",
    "datasets = {\n",
    "    'methods2test_runnable': '\\\\textsc{Methods2Test\\\\textsubscript{runnable}}',\n",
    "    'humaneval-x': '\\\\textsc{HumanEval-X\\\\textsubscript{java}}',\n",
    "}\n",
    "\n",
    "columns = ['valid_syntax', 'scores', 'coverage_runnable', 'coverage_instruction', 'coverage_branch']\n",
    "\n",
    "methods = {\n",
    "    'pre-trained': 'None',\n",
    "    'fine-tuning': 'Fine-tuning',\n",
    "    'lora': 'LoRA',\n",
    "    'ia3': '(IA)\\\\textsuperscript{3}',\n",
    "    'prompt-tuning': 'Prompt tuning',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588eedf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>ia3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>codegen-350M-multi</th>\n",
       "      <td>356,712,448</td>\n",
       "      <td>356,712,448</td>\n",
       "      <td>20,480</td>\n",
       "      <td>1,310,720</td>\n",
       "      <td>143,360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-1B_P</th>\n",
       "      <td>1,015,306,240</td>\n",
       "      <td>1,015,306,240</td>\n",
       "      <td>40,960</td>\n",
       "      <td>2,097,152</td>\n",
       "      <td>229,376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-3b</th>\n",
       "      <td>3,030,371,328</td>\n",
       "      <td>3,030,371,328</td>\n",
       "      <td>61,440</td>\n",
       "      <td>4,546,560</td>\n",
       "      <td>468,480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-3_7B_P</th>\n",
       "      <td>3,641,174,016</td>\n",
       "      <td>3,641,174,016</td>\n",
       "      <td>81,920</td>\n",
       "      <td>4,194,304</td>\n",
       "      <td>458,752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-7b-hf</th>\n",
       "      <td>6,738,546,688</td>\n",
       "      <td>6,738,546,688</td>\n",
       "      <td>81,920</td>\n",
       "      <td>8,388,608</td>\n",
       "      <td>614,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-7B_P</th>\n",
       "      <td>6,862,858,240</td>\n",
       "      <td>6,862,858,240</td>\n",
       "      <td>81,920</td>\n",
       "      <td>8,388,608</td>\n",
       "      <td>917,504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-7b</th>\n",
       "      <td>7,173,923,840</td>\n",
       "      <td>7,173,923,840</td>\n",
       "      <td>92,160</td>\n",
       "      <td>7,340,032</td>\n",
       "      <td>753,664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoderbase</th>\n",
       "      <td>15,517,456,384</td>\n",
       "      <td>15,517,456,384</td>\n",
       "      <td>122,880</td>\n",
       "      <td>8,028,160</td>\n",
       "      <td>1,239,040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-15b</th>\n",
       "      <td>15,655,899,136</td>\n",
       "      <td>15,655,899,136</td>\n",
       "      <td>122,880</td>\n",
       "      <td>12,124,160</td>\n",
       "      <td>1,249,280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-16B_P</th>\n",
       "      <td>16,032,155,648</td>\n",
       "      <td>16,032,155,648</td>\n",
       "      <td>122,880</td>\n",
       "      <td>13,369,344</td>\n",
       "      <td>1,462,272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       pre-trained     fine-tuning prompt-tuning        lora  \\\n",
       "model                                                                          \n",
       "codegen-350M-multi     356,712,448     356,712,448        20,480   1,310,720   \n",
       "codegen2-1B_P        1,015,306,240   1,015,306,240        40,960   2,097,152   \n",
       "starcoder2-3b        3,030,371,328   3,030,371,328        61,440   4,546,560   \n",
       "codegen2-3_7B_P      3,641,174,016   3,641,174,016        81,920   4,194,304   \n",
       "CodeLlama-7b-hf      6,738,546,688   6,738,546,688        81,920   8,388,608   \n",
       "codegen2-7B_P        6,862,858,240   6,862,858,240        81,920   8,388,608   \n",
       "starcoder2-7b        7,173,923,840   7,173,923,840        92,160   7,340,032   \n",
       "starcoderbase       15,517,456,384  15,517,456,384       122,880   8,028,160   \n",
       "starcoder2-15b      15,655,899,136  15,655,899,136       122,880  12,124,160   \n",
       "codegen2-16B_P      16,032,155,648  16,032,155,648       122,880  13,369,344   \n",
       "\n",
       "                          ia3  \n",
       "model                          \n",
       "codegen-350M-multi    143,360  \n",
       "codegen2-1B_P         229,376  \n",
       "starcoder2-3b         468,480  \n",
       "codegen2-3_7B_P       458,752  \n",
       "CodeLlama-7b-hf       614,400  \n",
       "codegen2-7B_P         917,504  \n",
       "starcoder2-7b         753,664  \n",
       "starcoderbase       1,239,040  \n",
       "starcoder2-15b      1,249,280  \n",
       "codegen2-16B_P      1,462,272  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainable_params =  pd.read_csv(DATA_DIR / 'params_data.csv', index_col=0)\n",
    "model_trainable_params.index = model_trainable_params.index.str.split('/').str[1]\n",
    "model_trainable_params = model_trainable_params.loc[models.keys()]\n",
    "model_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c89b0da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">methods2test_runnable</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">humaneval-x</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">valid_syntax</th>\n",
       "      <th colspan=\"5\" halign=\"left\">scores</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"5\" halign=\"left\">coverage_instruction</th>\n",
       "      <th colspan=\"5\" halign=\"left\">coverage_branch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>ia3</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>...</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>codegen-350M-multi</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.9505</td>\n",
       "      <td>0.9530</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.2897</td>\n",
       "      <td>0.2365</td>\n",
       "      <td>0.2286</td>\n",
       "      <td>0.2570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9733</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9869</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.9656</td>\n",
       "      <td>0.8977</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-1B_P</th>\n",
       "      <td>0.7527</td>\n",
       "      <td>0.7167</td>\n",
       "      <td>0.3694</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1434</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-3b</th>\n",
       "      <td>0.9687</td>\n",
       "      <td>0.8677</td>\n",
       "      <td>0.9805</td>\n",
       "      <td>0.9022</td>\n",
       "      <td>0.8592</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.2958</td>\n",
       "      <td>0.2801</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.3106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9958</td>\n",
       "      <td>0.8515</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>0.8327</td>\n",
       "      <td>0.7585</td>\n",
       "      <td>0.5673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-3_7B_P</th>\n",
       "      <td>0.4145</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.4106</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2571</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-7b-hf</th>\n",
       "      <td>0.9758</td>\n",
       "      <td>0.9661</td>\n",
       "      <td>0.9811</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.9749</td>\n",
       "      <td>0.3157</td>\n",
       "      <td>0.3151</td>\n",
       "      <td>0.3070</td>\n",
       "      <td>0.2968</td>\n",
       "      <td>0.3342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.9893</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.8612</td>\n",
       "      <td>0.8472</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>0.7247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-7B_P</th>\n",
       "      <td>0.9803</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>0.9787</td>\n",
       "      <td>0.9788</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.3030</td>\n",
       "      <td>0.2860</td>\n",
       "      <td>0.2795</td>\n",
       "      <td>0.3103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9831</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.8145</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.7950</td>\n",
       "      <td>0.6940</td>\n",
       "      <td>0.7535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-7b</th>\n",
       "      <td>0.9719</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.9741</td>\n",
       "      <td>0.9509</td>\n",
       "      <td>0.8473</td>\n",
       "      <td>0.1758</td>\n",
       "      <td>0.3045</td>\n",
       "      <td>0.3088</td>\n",
       "      <td>0.3236</td>\n",
       "      <td>0.3084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9906</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.9934</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.8289</td>\n",
       "      <td>0.9049</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.8578</td>\n",
       "      <td>0.8706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoderbase</th>\n",
       "      <td>0.9639</td>\n",
       "      <td>0.8363</td>\n",
       "      <td>0.8390</td>\n",
       "      <td>0.8390</td>\n",
       "      <td>0.8390</td>\n",
       "      <td>0.1748</td>\n",
       "      <td>0.3357</td>\n",
       "      <td>0.1748</td>\n",
       "      <td>0.1882</td>\n",
       "      <td>0.1748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>0.8148</td>\n",
       "      <td>0.7520</td>\n",
       "      <td>0.8148</td>\n",
       "      <td>0.6762</td>\n",
       "      <td>0.7389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-15b</th>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.8530</td>\n",
       "      <td>0.8530</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>0.3320</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>0.3297</td>\n",
       "      <td>0.3316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9893</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>0.8769</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>0.8789</td>\n",
       "      <td>0.8266</td>\n",
       "      <td>0.8262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-16B_P</th>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9788</td>\n",
       "      <td>0.9803</td>\n",
       "      <td>0.9763</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.2953</td>\n",
       "      <td>0.3255</td>\n",
       "      <td>0.2922</td>\n",
       "      <td>0.2904</td>\n",
       "      <td>0.3248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9866</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>0.9866</td>\n",
       "      <td>0.9746</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.8056</td>\n",
       "      <td>0.7807</td>\n",
       "      <td>0.8056</td>\n",
       "      <td>0.7107</td>\n",
       "      <td>0.8206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   methods2test_runnable                                \\\n",
       "                            valid_syntax                                 \n",
       "                             fine-tuning prompt-tuning    lora     ia3   \n",
       "model                                                                    \n",
       "codegen-350M-multi                0.9769        0.9505  0.9530  0.9420   \n",
       "codegen2-1B_P                     0.7527        0.7167  0.3694  0.0194   \n",
       "starcoder2-3b                     0.9687        0.8677  0.9805  0.9022   \n",
       "codegen2-3_7B_P                   0.4145        0.2250  0.4106  0.0000   \n",
       "CodeLlama-7b-hf                   0.9758        0.9661  0.9811  0.9769   \n",
       "codegen2-7B_P                     0.9803        0.9784  0.9795  0.9787   \n",
       "starcoder2-7b                     0.9719        0.8506  0.9741  0.9509   \n",
       "starcoderbase                     0.9639        0.8363  0.8390  0.8390   \n",
       "starcoder2-15b                    0.9800        0.9802  0.9778  0.8530   \n",
       "codegen2-16B_P                    0.9815        0.9788  0.9803  0.9763   \n",
       "\n",
       "                                                                              \\\n",
       "                                    scores                                     \n",
       "                   pre-trained pre-trained fine-tuning     ia3 prompt-tuning   \n",
       "model                                                                          \n",
       "codegen-350M-multi      0.9455      0.2310      0.2897  0.2365        0.2286   \n",
       "codegen2-1B_P           0.0000      0.0000      0.1434  0.2583        0.2572   \n",
       "starcoder2-3b           0.8592      0.1726      0.2958  0.2801        0.1742   \n",
       "codegen2-3_7B_P         0.0000      0.0000      0.1020  0.0000        0.2571   \n",
       "CodeLlama-7b-hf         0.9749      0.3157      0.3151  0.3070        0.2968   \n",
       "codegen2-7B_P           0.9788      0.2776      0.3030  0.2860        0.2795   \n",
       "starcoder2-7b           0.8473      0.1758      0.3045  0.3088        0.3236   \n",
       "starcoderbase           0.8390      0.1748      0.3357  0.1748        0.1882   \n",
       "starcoder2-15b          0.8530      0.1996      0.3320  0.1996        0.3297   \n",
       "codegen2-16B_P          0.9757      0.2953      0.3255  0.2922        0.2904   \n",
       "\n",
       "                            ...          humaneval-x                      \\\n",
       "                            ... coverage_instruction                       \n",
       "                      lora  ...          pre-trained fine-tuning     ia3   \n",
       "model                       ...                                            \n",
       "codegen-350M-multi  0.2570  ...               0.9733      1.0000  0.9869   \n",
       "codegen2-1B_P       0.0368  ...               0.0000      0.0000  0.0000   \n",
       "starcoder2-3b       0.3106  ...               0.9938      0.9888  0.9916   \n",
       "codegen2-3_7B_P     0.1025  ...               0.0000      0.0000  0.0000   \n",
       "CodeLlama-7b-hf     0.3342  ...               0.9830      0.9893  0.9877   \n",
       "codegen2-7B_P       0.3103  ...               0.9831      0.9975  0.9806   \n",
       "starcoder2-7b       0.3084  ...               0.9906      0.9912  0.9934   \n",
       "starcoderbase       0.1748  ...               0.9907      0.9916  0.9907   \n",
       "starcoder2-15b      0.3316  ...               0.9893      0.9950  0.9895   \n",
       "codegen2-16B_P      0.3248  ...               0.9866      0.9952  0.9866   \n",
       "\n",
       "                                                                              \\\n",
       "                                         coverage_branch                       \n",
       "                   prompt-tuning    lora     pre-trained fine-tuning     ia3   \n",
       "model                                                                          \n",
       "codegen-350M-multi        0.9769  0.9656          0.8977      0.8333  0.9432   \n",
       "codegen2-1B_P             0.0000  0.0000          0.0000      0.0000  0.0000   \n",
       "starcoder2-3b             0.9967  0.9958          0.8515      0.8520  0.8327   \n",
       "codegen2-3_7B_P           0.0000  0.0000          0.0000      0.0000  0.0000   \n",
       "CodeLlama-7b-hf           0.9838  0.9961          0.8417      0.8612  0.8472   \n",
       "codegen2-7B_P             0.9855  0.9910          0.8145      0.7000  0.7950   \n",
       "starcoder2-7b             0.9939  0.9938          0.8289      0.9049  0.8818   \n",
       "starcoderbase             0.9872  0.9976          0.8148      0.7520  0.8148   \n",
       "starcoder2-15b            0.9956  0.9931          0.8769      0.8096  0.8789   \n",
       "codegen2-16B_P            0.9746  0.9862          0.8056      0.7807  0.8056   \n",
       "\n",
       "                                          \n",
       "                                          \n",
       "                   prompt-tuning    lora  \n",
       "model                                     \n",
       "codegen-350M-multi        0.9167  0.8929  \n",
       "codegen2-1B_P             0.0000  0.0000  \n",
       "starcoder2-3b             0.7585  0.5673  \n",
       "codegen2-3_7B_P           0.0000  0.0000  \n",
       "CodeLlama-7b-hf           0.8225  0.7247  \n",
       "codegen2-7B_P             0.6940  0.7535  \n",
       "starcoder2-7b             0.8578  0.8706  \n",
       "starcoderbase             0.6762  0.7389  \n",
       "starcoder2-15b            0.8266  0.8262  \n",
       "codegen2-16B_P            0.7107  0.8206  \n",
       "\n",
       "[10 rows x 50 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    dataset_dir = DATA_DIR / dataset\n",
    "    \n",
    "    coverage_branch = pd.read_csv(dataset_dir / 'coverage_branch.csv', index_col=0)\n",
    "    coverage_branch.index = coverage_branch.index.str.split('/').str[1]\n",
    "    coverage_branch = coverage_branch.loc[models.keys()]\n",
    "    \n",
    "    coverage_instruction = pd.read_csv(dataset_dir / 'coverage_instruction.csv', index_col=0)\n",
    "    coverage_instruction.index = coverage_instruction.index.str.split('/').str[1]\n",
    "    coverage_instruction = coverage_instruction.loc[models.keys()]\n",
    "    \n",
    "    coverage_runnable = pd.read_csv(dataset_dir / 'coverage_runnable.csv', index_col=0)\n",
    "    coverage_runnable.index = coverage_runnable.index.str.split('/').str[1]\n",
    "    coverage_runnable = coverage_runnable.loc[models.keys()]\n",
    "    \n",
    "    \n",
    "    scores = pd.read_csv(dataset_dir / 'scores.csv', index_col=0)\n",
    "    scores.index = scores.index.str.split('/').str[1]\n",
    "    scores = scores.loc[models.keys()]\n",
    "    \n",
    "    valid_syntax = pd.read_csv(dataset_dir / 'valid_syntax.csv', index_col=0)\n",
    "    valid_syntax.index = valid_syntax.index.str.split('/').str[1]\n",
    "    valid_syntax = valid_syntax.loc[models.keys()]\n",
    "    \n",
    "    data[dataset] = pd.concat([valid_syntax, scores, coverage_runnable, coverage_instruction, coverage_branch], axis=1, keys=columns)\n",
    "    \n",
    "\n",
    "#concat all datasets\n",
    "data = pd.concat(data, axis=1)\n",
    "data = data.astype(float).round(4)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08418048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'codegen-350M-multi': [('methods2test_runnable',\n",
       "   'valid_syntax',\n",
       "   'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3')],\n",
       " 'codegen2-1B_P': [('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'starcoder2-3b': [('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'scores', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning')],\n",
       " 'codegen2-3_7B_P': [('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'CodeLlama-7b-hf': [('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'scores', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning')],\n",
       " 'codegen2-7B_P': [('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained')],\n",
       " 'starcoder2-7b': [('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'scores', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning')],\n",
       " 'starcoderbase': [('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3')],\n",
       " 'starcoder2-15b': [('methods2test_runnable', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3')],\n",
       " 'codegen2-16B_P': [('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_method_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for column in columns:\n",
    "        for model in models.keys():\n",
    "            model_data = data[dataset, column].loc[model]\n",
    "            # Find max value and what method it corresponds to\n",
    "            max_value = model_data.max()\n",
    "            max_indices = model_data[model_data == max_value].index.tolist()\n",
    "            # If all values are the same, max_indices will be empty\n",
    "            #if set(max_indices) == set(model_data.index.tolist()):\n",
    "            #    print(f\"All values are the same for {dataset}, {column}, {model}.\")\n",
    "            #    continue\n",
    "            \n",
    "            for method_index in max_indices:\n",
    "                best_method_data.setdefault(model, [])\n",
    "                best_method_data[model].append((dataset, column, method_index))\n",
    "                \n",
    "best_method_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a631b8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'codegen-350M-multi': [('methods2test_runnable', 'valid_syntax', 'ia3'),\n",
       "  ('methods2test_runnable', 'scores', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'CodeLlama-7b-hf': [('methods2test_runnable',\n",
       "   'valid_syntax',\n",
       "   'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'ia3'),\n",
       "  ('methods2test_runnable', 'scores', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'codegen2-7B_P': [('methods2test_runnable', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'starcoderbase': [('methods2test_runnable', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'codegen2-16B_P': [('methods2test_runnable', 'scores', 'ia3'),\n",
       "  ('methods2test_runnable', 'scores', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning')],\n",
       " 'starcoder2-3b': [('methods2test_runnable',\n",
       "   'coverage_runnable',\n",
       "   'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'starcoder2-7b': [('methods2test_runnable',\n",
       "   'coverage_runnable',\n",
       "   'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora')],\n",
       " 'starcoder2-15b': [('methods2test_runnable',\n",
       "   'coverage_runnable',\n",
       "   'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decreased_performance_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for column in columns:\n",
    "        for model in models.keys():\n",
    "            model_data = data[dataset, column].loc[model]\n",
    "            # Find max value and what method it corresponds to\n",
    "            baseline_value = model_data.loc['pre-trained']\n",
    "            \n",
    "            decreased_indices = model_data[model_data < baseline_value].index.tolist()\n",
    "            \n",
    "            for method_index in decreased_indices:\n",
    "                decreased_performance_data.setdefault(model, [])\n",
    "                decreased_performance_data[model].append((dataset, column, method_index))\n",
    "\n",
    "decreased_performance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55bf7d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'codegen2-1B_P': [('methods2test_runnable', 'valid_syntax', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'scores', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'scores', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'lora'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'ia3'),\n",
       "  ('methods2test_runnable', 'scores', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'scores', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning')],\n",
       " 'codegen2-3_7B_P': [('methods2test_runnable', 'valid_syntax', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'scores', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'pre-trained'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'fine-tuning'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'lora'),\n",
       "  ('methods2test_runnable', 'scores', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'lora'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'lora'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'ia3'),\n",
       "  ('methods2test_runnable', 'scores', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'ia3'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'ia3'),\n",
       "  ('methods2test_runnable', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'scores', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('methods2test_runnable', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'scores', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_syntactical_performance_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for method in methods.keys():\n",
    "        for model in models.keys():\n",
    "            valid_syntax = data[dataset, \"valid_syntax\", method].loc[model]\n",
    "            if valid_syntax < 0.5:\n",
    "                for column in columns:  # Skip 'valid_syntax'\n",
    "                    bad_syntactical_performance_data.setdefault(model, [])\n",
    "                    bad_syntactical_performance_data[model].append((dataset, column, method))\n",
    "                    \n",
    "\n",
    "bad_syntactical_performance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae2776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_latex(text):\n",
    "    \"\"\"Escape LaTeX special characters.\"\"\"\n",
    "    return text.replace('_', '\\\\_').replace('%', '\\\\%').replace('&', '\\\\&').replace('$', '\\\\$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d702838",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "\n",
    "table.append(\"\\\\begin{table*}[htbp]\")\n",
    "table.append(\"\\\\begin{threeparttable}\")\n",
    "table.append(\"    \\\\newcolumntype{Y}{>{\\\\centering\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\newcolumntype{R}{>{\\\\raggedright\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\newcolumntype{L}{>{\\\\raggedleft\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\centering\")\n",
    "table.append(\"    \\\\footnotesize\")\n",
    "table.append(\"    \\\\caption{Comparison of syntactical validity and CodeBLEU scores from experiments using different tuning methods across various models on testing split of \\\\textsc{Methods2Test\\\\textsubscript{runnable}} and \\textsc{HumanEval-X\\\\textsubscript{java}} datasets. }\\\\label{tab:eval-summary}\")\n",
    "\n",
    "row = \"    \\\\begin{tabularx}{\\\\textwidth}{lr\"\n",
    "for i, dataset in enumerate(datasets, start=1):\n",
    "    row += \"L\" * 5  # 5 columns for each dataset\n",
    "    if i < len(datasets):\n",
    "        row += \"!{\\\\color{white}\\\\ }\"  # Add a space for the vertical line\n",
    "row += \"}\"\n",
    "table.append(row)\n",
    "#table.append(\"    \\\\begin{tabularx}{\\\\textwidth}{lrLLLLL!{\\\\color{white}\\\\ }LLLLL}\")\n",
    "\n",
    "\n",
    "table.append(\"        \\\\toprule\")\n",
    "\n",
    "row = \"        \\\\multirow{2}{*}{\\\\textbf{Method}} & \\\\multirow{3}{*}{\\\\parbox[t]{1cm}{\\\\centering \\\\textbf{Trainable\\\\\\\\params}}}\"\n",
    "for dataset in datasets:\n",
    "    row += \" & \\\\multicolumn{5}{c}{\\\\textbf{\" + datasets[dataset] + \"}}\"\n",
    "row += \"\\\\\\\\\"\n",
    "table.append(row)\n",
    "\n",
    "#\\cmidrule(lr){3-7}\\cmidrule(lr){8-12}\n",
    "row = \"        \"\n",
    "for i, dataset in enumerate(datasets):\n",
    "    index = 3 + (i * 5)\n",
    "    row += \"\\\\cmidrule(lr){\" + f\"{index}-{index + 4}\" + \"}\"\n",
    "table.append(row)\n",
    "\n",
    "row  = \"        &\"\n",
    "for dataset in datasets:\n",
    "    row += \" & \\\\rotatebox[origin=l]{90}{Valid syntax} & \\\\rotatebox[origin=l]{90}{CodeBLEU} & \\\\rotatebox[origin=l]{90}{pass@1} & \\\\rotatebox[origin=l]{90}{Instr Cov} & \\\\rotatebox[origin=l]{90}{Branch Cov}\"\n",
    "row += \"\\\\\\\\\"\n",
    "table.append(row)\n",
    "\n",
    "table.append(\"        \\\\hline\")\n",
    "\n",
    "\n",
    "for model in models.keys():\n",
    "    table.append(\"        \\\\multicolumn{\" + str(2+5*len(datasets)) + \"}{l}{\\\\cellcolor{gray!10}{\\\\textbf{\" + models[model] + \"}}} \\\\bigstrut \\\\\\\\*\")\n",
    "    for method in methods:\n",
    "        col = []\n",
    "        for dataset in datasets:\n",
    "            for column in columns:\n",
    "                try:\n",
    "                    value = data[dataset, column, method].loc[model]\n",
    "                except KeyError:\n",
    "                    value = \"N/A\"\n",
    "            \n",
    "                if (dataset, column, method) in decreased_performance_data.get(model, []):\n",
    "                    value = f\"({value})\"\n",
    "                    \n",
    "                if (dataset, column, method) in best_method_data.get(model, []):\n",
    "                    value = f\"\\\\textbf{{{value}}}\"\n",
    "                    \n",
    "                if (dataset, column, method) in bad_syntactical_performance_data.get(model, []):\n",
    "                    value = f\"\\\\cellcolor{{red!10}}{{{value}}}\"\n",
    "                    \n",
    "                col.append(f\"{value}\")\n",
    "        row = \" & \".join(col)\n",
    "        params = model_trainable_params[method].loc[model]\n",
    "        row = \"        \" + methods[method] + \" & \" + params + \" & \" + row + \" \\\\\\\\\"\n",
    "        table.append(row)\n",
    "    table.append(\"\")\n",
    "\n",
    "table.append(\"       \\\\bottomrule\")\n",
    "table.append(\"    \\\\end{tabularx}\")\n",
    "table.append(\"    \\\\begin{tablenotes}[flushleft]\\\\small\")\n",
    "table.append(\"      \\\\item \\\\textbf{Bold}: best-performing training method per model. (Parentheses): decreased performance compared to baseline. \\\\colorbox{red!10}{Red}: $<$ 50\\\\% syntactical valid samples. \\\\underline{Underline}: Other notable results (see in \\\\Cref{sec:syntax}).\")\n",
    "table.append(\"    \\\\end{tablenotes}\")\n",
    "table.append(\"\\\\end{threeparttable}\")\n",
    "table.append(\"\\\\end{table*}\")\n",
    "\n",
    "#print(\"\\n\".join(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "720e0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"\\n\".join(table)\n",
    "table_path = Path.cwd().parent / 'tables' / 'eval_summary.tex'\n",
    "table_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(table_path, 'w') as f:\n",
    "    f.write(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
