{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c2f66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbc63bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'codegen-350M-multi': 'CodeGen-350M-multi',\n",
    "    'codegen2-1B_P': 'CodeGen2-1B',\n",
    "    'starcoder2-3b': 'StarCoder2-3B',\n",
    "    'codegen2-3_7B_P': 'CodeGen2-3.7B',\n",
    "    'CodeLlama-7b-hf': 'CodeLlama-7B',\n",
    "    'codegen2-7B_P': 'CodeGen2-7B',\n",
    "    'starcoder2-7b': 'StarCoder2-7B',\n",
    "    'starcoderbase': 'StarCoderBase',\n",
    "    'starcoder2-15b': 'StarCoder2-15B',\n",
    "    'codegen2-16B_P': 'CodeGen2-16B',\n",
    "}\n",
    "\n",
    "#datasets = ['methods2test_runnable', 'humaneval-x']\n",
    "datasets = {\n",
    "    'humaneval-x': '\\\\textsc{HumanEval-X\\\\textsubscript{java}}',\n",
    "#    'methods2test_runnable': '\\\\textsc{Methods2Test\\\\textsubscript{runnable}}',\n",
    "}\n",
    "\n",
    "columns = ['valid_syntax', 'scores', 'coverage_runnable', 'coverage_instruction', 'coverage_branch']\n",
    "\n",
    "methods = {\n",
    "    'pre-trained': 'None',\n",
    "    'fine-tuning': 'Fine-tuning',\n",
    "    'prompt-tuning': 'Prompt tuning',\n",
    "    'lora': 'LoRA',\n",
    "    'ia3': '(IA)\\\\textsuperscript{3}',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "588eedf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>ia3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>codegen-350M-multi</th>\n",
       "      <td>356,712,448</td>\n",
       "      <td>356,712,448</td>\n",
       "      <td>20,480</td>\n",
       "      <td>1,310,720</td>\n",
       "      <td>143,360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-1B_P</th>\n",
       "      <td>1,015,306,240</td>\n",
       "      <td>1,015,306,240</td>\n",
       "      <td>40,960</td>\n",
       "      <td>2,097,152</td>\n",
       "      <td>229,376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-3b</th>\n",
       "      <td>3,030,371,328</td>\n",
       "      <td>3,030,371,328</td>\n",
       "      <td>61,440</td>\n",
       "      <td>4,546,560</td>\n",
       "      <td>468,480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-3_7B_P</th>\n",
       "      <td>3,641,174,016</td>\n",
       "      <td>3,641,174,016</td>\n",
       "      <td>81,920</td>\n",
       "      <td>4,194,304</td>\n",
       "      <td>458,752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-7b-hf</th>\n",
       "      <td>6,738,546,688</td>\n",
       "      <td>6,738,546,688</td>\n",
       "      <td>81,920</td>\n",
       "      <td>8,388,608</td>\n",
       "      <td>614,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-7B_P</th>\n",
       "      <td>6,862,858,240</td>\n",
       "      <td>6,862,858,240</td>\n",
       "      <td>81,920</td>\n",
       "      <td>8,388,608</td>\n",
       "      <td>917,504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-7b</th>\n",
       "      <td>7,173,923,840</td>\n",
       "      <td>7,173,923,840</td>\n",
       "      <td>92,160</td>\n",
       "      <td>7,340,032</td>\n",
       "      <td>753,664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoderbase</th>\n",
       "      <td>15,517,456,384</td>\n",
       "      <td>15,517,456,384</td>\n",
       "      <td>122,880</td>\n",
       "      <td>8,028,160</td>\n",
       "      <td>1,239,040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-15b</th>\n",
       "      <td>15,655,899,136</td>\n",
       "      <td>15,655,899,136</td>\n",
       "      <td>122,880</td>\n",
       "      <td>12,124,160</td>\n",
       "      <td>1,249,280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-16B_P</th>\n",
       "      <td>16,032,155,648</td>\n",
       "      <td>16,032,155,648</td>\n",
       "      <td>122,880</td>\n",
       "      <td>13,369,344</td>\n",
       "      <td>1,462,272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       pre-trained     fine-tuning prompt-tuning        lora  \\\n",
       "model                                                                          \n",
       "codegen-350M-multi     356,712,448     356,712,448        20,480   1,310,720   \n",
       "codegen2-1B_P        1,015,306,240   1,015,306,240        40,960   2,097,152   \n",
       "starcoder2-3b        3,030,371,328   3,030,371,328        61,440   4,546,560   \n",
       "codegen2-3_7B_P      3,641,174,016   3,641,174,016        81,920   4,194,304   \n",
       "CodeLlama-7b-hf      6,738,546,688   6,738,546,688        81,920   8,388,608   \n",
       "codegen2-7B_P        6,862,858,240   6,862,858,240        81,920   8,388,608   \n",
       "starcoder2-7b        7,173,923,840   7,173,923,840        92,160   7,340,032   \n",
       "starcoderbase       15,517,456,384  15,517,456,384       122,880   8,028,160   \n",
       "starcoder2-15b      15,655,899,136  15,655,899,136       122,880  12,124,160   \n",
       "codegen2-16B_P      16,032,155,648  16,032,155,648       122,880  13,369,344   \n",
       "\n",
       "                          ia3  \n",
       "model                          \n",
       "codegen-350M-multi    143,360  \n",
       "codegen2-1B_P         229,376  \n",
       "starcoder2-3b         468,480  \n",
       "codegen2-3_7B_P       458,752  \n",
       "CodeLlama-7b-hf       614,400  \n",
       "codegen2-7B_P         917,504  \n",
       "starcoder2-7b         753,664  \n",
       "starcoderbase       1,239,040  \n",
       "starcoder2-15b      1,249,280  \n",
       "codegen2-16B_P      1,462,272  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainable_params =  pd.read_csv(DATA_DIR / 'params_data.csv', index_col=0)\n",
    "model_trainable_params.index = model_trainable_params.index.str.split('/').str[1]\n",
    "model_trainable_params = model_trainable_params.loc[models.keys()]\n",
    "model_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c89b0da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"21\" halign=\"left\">humaneval-x</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">valid_syntax</th>\n",
       "      <th colspan=\"5\" halign=\"left\">scores</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"5\" halign=\"left\">coverage_instruction</th>\n",
       "      <th colspan=\"5\" halign=\"left\">coverage_branch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>ia3</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>...</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "      <th>fine-tuning</th>\n",
       "      <th>pre-trained</th>\n",
       "      <th>ia3</th>\n",
       "      <th>prompt-tuning</th>\n",
       "      <th>lora</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>codegen-350M-multi</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3603</td>\n",
       "      <td>0.3291</td>\n",
       "      <td>0.3591</td>\n",
       "      <td>0.3274</td>\n",
       "      <td>0.3906</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9733</td>\n",
       "      <td>0.9869</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.9656</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.8977</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-1B_P</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0793</td>\n",
       "      <td>0.0854</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2547</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-3b</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4273</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4317</td>\n",
       "      <td>0.4170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9919</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.8549</td>\n",
       "      <td>0.8567</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.7585</td>\n",
       "      <td>0.5884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-3_7B_P</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4024</td>\n",
       "      <td>0.7378</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2621</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-7b-hf</th>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.4807</td>\n",
       "      <td>0.4623</td>\n",
       "      <td>0.4297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>0.8637</td>\n",
       "      <td>0.8446</td>\n",
       "      <td>0.8472</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>0.7247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-7B_P</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4740</td>\n",
       "      <td>0.4407</td>\n",
       "      <td>0.4906</td>\n",
       "      <td>0.4409</td>\n",
       "      <td>0.4645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.9831</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.8145</td>\n",
       "      <td>0.7950</td>\n",
       "      <td>0.6940</td>\n",
       "      <td>0.7535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-7b</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4030</td>\n",
       "      <td>0.4398</td>\n",
       "      <td>0.4216</td>\n",
       "      <td>0.5058</td>\n",
       "      <td>0.5186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9935</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.9080</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>0.8839</td>\n",
       "      <td>0.8602</td>\n",
       "      <td>0.8706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoderbase</th>\n",
       "      <td>0.9878</td>\n",
       "      <td>0.7805</td>\n",
       "      <td>0.9878</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9878</td>\n",
       "      <td>0.4342</td>\n",
       "      <td>0.4832</td>\n",
       "      <td>0.4343</td>\n",
       "      <td>0.2312</td>\n",
       "      <td>0.3911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>0.7520</td>\n",
       "      <td>0.8148</td>\n",
       "      <td>0.8148</td>\n",
       "      <td>0.6762</td>\n",
       "      <td>0.7389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starcoder2-15b</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3728</td>\n",
       "      <td>0.4896</td>\n",
       "      <td>0.3729</td>\n",
       "      <td>0.5349</td>\n",
       "      <td>0.4646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>0.9893</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>0.9932</td>\n",
       "      <td>0.8130</td>\n",
       "      <td>0.8769</td>\n",
       "      <td>0.8789</td>\n",
       "      <td>0.8289</td>\n",
       "      <td>0.8288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codegen2-16B_P</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4793</td>\n",
       "      <td>0.3783</td>\n",
       "      <td>0.4795</td>\n",
       "      <td>0.4676</td>\n",
       "      <td>0.4714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>0.9866</td>\n",
       "      <td>0.9866</td>\n",
       "      <td>0.9746</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.7807</td>\n",
       "      <td>0.8056</td>\n",
       "      <td>0.8056</td>\n",
       "      <td>0.7107</td>\n",
       "      <td>0.8206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    humaneval-x                                            \\\n",
       "                   valid_syntax                                             \n",
       "                    pre-trained prompt-tuning    lora fine-tuning     ia3   \n",
       "model                                                                       \n",
       "codegen-350M-multi       1.0000        1.0000  1.0000      1.0000  1.0000   \n",
       "codegen2-1B_P            0.0000        0.0793  0.0854      0.0549  0.0000   \n",
       "starcoder2-3b            1.0000        1.0000  1.0000      1.0000  1.0000   \n",
       "codegen2-3_7B_P          0.0000        0.0000  0.4024      0.7378  0.0000   \n",
       "CodeLlama-7b-hf          0.9939        0.9939  0.9939      1.0000  1.0000   \n",
       "codegen2-7B_P            1.0000        1.0000  1.0000      1.0000  1.0000   \n",
       "starcoder2-7b            1.0000        1.0000  1.0000      1.0000  1.0000   \n",
       "starcoderbase            0.9878        0.7805  0.9878      1.0000  0.9878   \n",
       "starcoder2-15b           1.0000        1.0000  1.0000      0.9939  1.0000   \n",
       "codegen2-16B_P           1.0000        1.0000  1.0000      0.9817  1.0000   \n",
       "\n",
       "                                                                          ...  \\\n",
       "                        scores                                            ...   \n",
       "                   pre-trained fine-tuning     ia3 prompt-tuning    lora  ...   \n",
       "model                                                                     ...   \n",
       "codegen-350M-multi      0.3603      0.3291  0.3591        0.3274  0.3906  ...   \n",
       "codegen2-1B_P           0.0000      0.0359  0.0000        0.2547  0.0117  ...   \n",
       "starcoder2-3b           0.4273      0.4974  0.4245        0.4317  0.4170  ...   \n",
       "codegen2-3_7B_P         0.0000      0.2621  0.0000        0.0000  0.1384  ...   \n",
       "CodeLlama-7b-hf         0.4870      0.5002  0.4807        0.4623  0.4297  ...   \n",
       "codegen2-7B_P           0.4740      0.4407  0.4906        0.4409  0.4645  ...   \n",
       "starcoder2-7b           0.4030      0.4398  0.4216        0.5058  0.5186  ...   \n",
       "starcoderbase           0.4342      0.4832  0.4343        0.2312  0.3911  ...   \n",
       "starcoder2-15b          0.3728      0.4896  0.3729        0.5349  0.4646  ...   \n",
       "codegen2-16B_P          0.4793      0.3783  0.4795        0.4676  0.4714  ...   \n",
       "\n",
       "                                                                           \\\n",
       "                   coverage_instruction                                     \n",
       "                            fine-tuning pre-trained     ia3 prompt-tuning   \n",
       "model                                                                       \n",
       "codegen-350M-multi               1.0000      0.9733  0.9869        0.9769   \n",
       "codegen2-1B_P                    0.0000      0.0000  0.0000        0.0000   \n",
       "starcoder2-3b                    0.9890      0.9940  0.9919        0.9967   \n",
       "codegen2-3_7B_P                  0.0000      0.0000  0.0000        0.0000   \n",
       "CodeLlama-7b-hf                  0.9895      0.9833  0.9877        0.9838   \n",
       "codegen2-7B_P                    0.9975      0.9831  0.9806        0.9855   \n",
       "starcoder2-7b                    0.9915      0.9907  0.9935        0.9940   \n",
       "starcoderbase                    0.9916      0.9907  0.9907        0.9872   \n",
       "starcoder2-15b                   0.9952      0.9893  0.9895        0.9957   \n",
       "codegen2-16B_P                   0.9952      0.9866  0.9866        0.9746   \n",
       "\n",
       "                                                                              \\\n",
       "                           coverage_branch                                     \n",
       "                      lora     fine-tuning pre-trained     ia3 prompt-tuning   \n",
       "model                                                                          \n",
       "codegen-350M-multi  0.9656          0.8333      0.8977  0.9432        0.9167   \n",
       "codegen2-1B_P       0.0000          0.0000      0.0000  0.0000        0.0000   \n",
       "starcoder2-3b       0.9960          0.8549      0.8567  0.8381        0.7585   \n",
       "codegen2-3_7B_P     0.0000          0.0000      0.0000  0.0000        0.0000   \n",
       "CodeLlama-7b-hf     0.9961          0.8637      0.8446  0.8472        0.8225   \n",
       "codegen2-7B_P       0.9910          0.7000      0.8145  0.7950        0.6940   \n",
       "starcoder2-7b       0.9938          0.9080      0.8316  0.8839        0.8602   \n",
       "starcoderbase       0.9976          0.7520      0.8148  0.8148        0.6762   \n",
       "starcoder2-15b      0.9932          0.8130      0.8769  0.8789        0.8289   \n",
       "codegen2-16B_P      0.9862          0.7807      0.8056  0.8056        0.7107   \n",
       "\n",
       "                            \n",
       "                            \n",
       "                      lora  \n",
       "model                       \n",
       "codegen-350M-multi  0.8929  \n",
       "codegen2-1B_P       0.0000  \n",
       "starcoder2-3b       0.5884  \n",
       "codegen2-3_7B_P     0.0000  \n",
       "CodeLlama-7b-hf     0.7247  \n",
       "codegen2-7B_P       0.7535  \n",
       "starcoder2-7b       0.8706  \n",
       "starcoderbase       0.7389  \n",
       "starcoder2-15b      0.8288  \n",
       "codegen2-16B_P      0.8206  \n",
       "\n",
       "[10 rows x 25 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    dataset_dir = DATA_DIR / dataset\n",
    "    \n",
    "    coverage_branch = pd.read_csv(dataset_dir / 'coverage_branch.csv', index_col=0, header=None).T.set_index('model').loc[models.keys()]\n",
    "    \n",
    "    \n",
    "    coverage_instruction = pd.read_csv(dataset_dir / 'coverage_instruction.csv', index_col=0, header=None).T.set_index('model').loc[models.keys()]\n",
    "    \n",
    "    coverage_runnable = pd.read_csv(dataset_dir / 'coverage_runnable.csv', index_col=0, header=None).T.set_index('model').loc[models.keys()]\n",
    "    \n",
    "    scores = pd.read_csv(dataset_dir / 'scores.csv', index_col=0)\n",
    "    scores.index = scores.index.str.split('/').str[1]\n",
    "    scores = scores.loc[models.keys()]\n",
    "    \n",
    "    valid_syntax = pd.read_csv(dataset_dir / 'valid_syntax.csv', index_col=0)\n",
    "    valid_syntax.index = valid_syntax.index.str.split('/').str[1]\n",
    "    valid_syntax = valid_syntax.loc[models.keys()]\n",
    "    \n",
    "    data[dataset] = pd.concat([valid_syntax, scores, coverage_runnable, coverage_instruction, coverage_branch], axis=1, keys=columns)\n",
    "    \n",
    "\n",
    "#concat all datasets\n",
    "data = pd.concat(data, axis=1)\n",
    "data = data.astype(float).round(4)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "08418048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'codegen-350M-multi': [('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3')],\n",
       " 'codegen2-1B_P': [('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'starcoder2-3b': [('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained')],\n",
       " 'codegen2-3_7B_P': [('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'CodeLlama-7b-hf': [('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning')],\n",
       " 'codegen2-7B_P': [('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained')],\n",
       " 'starcoder2-7b': [('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning')],\n",
       " 'starcoderbase': [('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3')],\n",
       " 'starcoder2-15b': [('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3')],\n",
       " 'codegen2-16B_P': [('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_method_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for column in columns:\n",
    "        for model in models.keys():\n",
    "            model_data = data[dataset, column].loc[model]\n",
    "            # Find max value and what method it corresponds to\n",
    "            max_value = model_data.max()\n",
    "            max_indices = model_data[model_data == max_value].index.tolist()\n",
    "            # If all values are the same, max_indices will be empty\n",
    "            #if set(max_indices) == set(model_data.index.tolist()):\n",
    "            #    print(f\"All values are the same for {dataset}, {column}, {model}.\")\n",
    "            #    continue\n",
    "            \n",
    "            for method_index in max_indices:\n",
    "                best_method_data.setdefault(model, [])\n",
    "                best_method_data[model].append((dataset, column, method_index))\n",
    "                \n",
    "best_method_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a631b8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'starcoderbase': [('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'starcoder2-15b': [('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'codegen2-16B_P': [('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning')],\n",
       " 'codegen-350M-multi': [('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'starcoder2-3b': [('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'CodeLlama-7b-hf': [('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'codegen2-7B_P': [('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora')],\n",
       " 'starcoder2-7b': [('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora')]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decreased_performance_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for column in columns:\n",
    "        for model in models.keys():\n",
    "            model_data = data[dataset, column].loc[model]\n",
    "            # Find max value and what method it corresponds to\n",
    "            baseline_value = model_data.loc['pre-trained']\n",
    "            \n",
    "            decreased_indices = model_data[model_data < baseline_value].index.tolist()\n",
    "            \n",
    "            for method_index in decreased_indices:\n",
    "                decreased_performance_data.setdefault(model, [])\n",
    "                decreased_performance_data[model].append((dataset, column, method_index))\n",
    "\n",
    "decreased_performance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55bf7d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'codegen2-1B_P': [('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'scores', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'fine-tuning'),\n",
       "  ('humaneval-x', 'scores', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'fine-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'fine-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3')],\n",
       " 'codegen2-3_7B_P': [('humaneval-x', 'valid_syntax', 'pre-trained'),\n",
       "  ('humaneval-x', 'scores', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'pre-trained'),\n",
       "  ('humaneval-x', 'coverage_branch', 'pre-trained'),\n",
       "  ('humaneval-x', 'valid_syntax', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'scores', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'coverage_branch', 'prompt-tuning'),\n",
       "  ('humaneval-x', 'valid_syntax', 'lora'),\n",
       "  ('humaneval-x', 'scores', 'lora'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'lora'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'lora'),\n",
       "  ('humaneval-x', 'coverage_branch', 'lora'),\n",
       "  ('humaneval-x', 'valid_syntax', 'ia3'),\n",
       "  ('humaneval-x', 'scores', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_runnable', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_instruction', 'ia3'),\n",
       "  ('humaneval-x', 'coverage_branch', 'ia3')]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_syntactical_performance_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for method in methods.keys():\n",
    "        for model in models.keys():\n",
    "            valid_syntax = data[dataset, \"valid_syntax\", method].loc[model]\n",
    "            if valid_syntax < 0.5:\n",
    "                for column in columns:  # Skip 'valid_syntax'\n",
    "                    bad_syntactical_performance_data.setdefault(model, [])\n",
    "                    bad_syntactical_performance_data[model].append((dataset, column, method))\n",
    "                    \n",
    "\n",
    "bad_syntactical_performance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ae2776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_latex(text):\n",
    "    \"\"\"Escape LaTeX special characters.\"\"\"\n",
    "    return text.replace('_', '\\\\_').replace('%', '\\\\%').replace('&', '\\\\&').replace('$', '\\\\$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d702838",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "\n",
    "table.append(\"\\\\begin{table*}[htbp]\")\n",
    "table.append(\"\\\\begin{threeparttable}\")\n",
    "table.append(\"    \\\\newcolumntype{Y}{>{\\\\centering\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\newcolumntype{R}{>{\\\\raggedright\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\newcolumntype{L}{>{\\\\raggedleft\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\centering\")\n",
    "table.append(\"    \\\\footnotesize\")\n",
    "table.append(\"    \\\\caption{Comparison of syntactical validity and CodeBLEU scores from experiments using different tuning methods across various models on testing split of \\\\textsc{Methods2Test\\\\textsubscript{runnable}} and \\textsc{HumanEval-X\\\\textsubscript{java}} datasets. }\\\\label{tab:eval-summary}\")\n",
    "\n",
    "row = \"    \\\\begin{tabularx}{\\\\textwidth}{lr\"\n",
    "for i, dataset in enumerate(datasets, start=1):\n",
    "    row += \"L\" * 5  # 5 columns for each dataset\n",
    "    if i < len(datasets):\n",
    "        row += \"!{\\\\color{white}\\\\ }\"  # Add a space for the vertical line\n",
    "row += \"}\"\n",
    "table.append(row)\n",
    "#table.append(\"    \\\\begin{tabularx}{\\\\textwidth}{lrLLLLL!{\\\\color{white}\\\\ }LLLLL}\")\n",
    "\n",
    "\n",
    "table.append(\"        \\\\toprule\")\n",
    "\n",
    "row = \"        \\\\multirow{2}{*}{\\\\textbf{Method}} & \\\\multirow{3}{*}{\\\\parbox[t]{1cm}{\\\\centering \\\\textbf{Trainable\\\\\\\\params}}}\"\n",
    "for dataset in datasets:\n",
    "    row += \" & \\\\multicolumn{5}{c}{\\\\textbf{\" + datasets[dataset] + \"}}\"\n",
    "row += \"\\\\\\\\\"\n",
    "table.append(row)\n",
    "\n",
    "#\\cmidrule(lr){3-7}\\cmidrule(lr){8-12}\n",
    "row = \"        \"\n",
    "for i, dataset in enumerate(datasets):\n",
    "    index = 3 + (i * 5)\n",
    "    row += \"\\\\cmidrule(lr){\" + f\"{index}-{index + 4}\" + \"}\"\n",
    "table.append(row)\n",
    "\n",
    "row  = \"        &\"\n",
    "for dataset in datasets:\n",
    "    row += \" & \\\\rotatebox[origin=l]{90}{Valid syntax} & \\\\rotatebox[origin=l]{90}{CodeBLEU} & \\\\rotatebox[origin=l]{90}{pass@1} & \\\\rotatebox[origin=l]{90}{Instr Cov} & \\\\rotatebox[origin=l]{90}{Branch Cov}\"\n",
    "row += \"\\\\\\\\\"\n",
    "table.append(row)\n",
    "\n",
    "table.append(\"        \\\\hline\")\n",
    "\n",
    "\n",
    "for model in models.keys():\n",
    "    table.append(\"        \\\\multicolumn{\" + str(2+5*len(datasets)) + \"}{l}{\\\\cellcolor{gray!10}{\\\\textbf{\" + models[model] + \"}}} \\\\bigstrut \\\\\\\\*\")\n",
    "    for method in methods:\n",
    "        col = []\n",
    "        for dataset in datasets:\n",
    "            for column in columns:\n",
    "                try:\n",
    "                    value = data[dataset, column, method].loc[model]\n",
    "                except KeyError:\n",
    "                    value = \"N/A\"\n",
    "            \n",
    "                if (dataset, column, method) in decreased_performance_data.get(model, []):\n",
    "                    value = f\"({value})\"\n",
    "                    \n",
    "                if (dataset, column, method) in best_method_data.get(model, []):\n",
    "                    value = f\"\\\\textbf{{{value}}}\"\n",
    "                    \n",
    "                if (dataset, column, method) in bad_syntactical_performance_data.get(model, []):\n",
    "                    value = f\"\\\\cellcolor{{red!10}}{{{value}}}\"\n",
    "                    \n",
    "                col.append(f\"{value}\")\n",
    "        row = \" & \".join(col)\n",
    "        params = model_trainable_params[method].loc[model]\n",
    "        row = \"        \" + methods[method] + \" & \" + params + \" & \" + row + \" \\\\\\\\\"\n",
    "        table.append(row)\n",
    "    table.append(\"\")\n",
    "\n",
    "table.append(\"       \\\\bottomrule\")\n",
    "table.append(\"    \\\\end{tabularx}\")\n",
    "table.append(\"    \\\\begin{tablenotes}[flushleft]\\\\small\")\n",
    "table.append(\"      \\\\item \\\\textbf{Bold}: best-performing training method per model. (Parentheses): decreased performance compared to baseline. \\\\colorbox{red!10}{Red}: $<$ 50\\\\% syntactical valid samples. \\\\underline{Underline}: Other notable results (see in \\\\Cref{sec:syntax}).\")\n",
    "table.append(\"    \\\\end{tablenotes}\")\n",
    "table.append(\"\\\\end{threeparttable}\")\n",
    "table.append(\"\\\\end{table*}\")\n",
    "\n",
    "#print(\"\\n\".join(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "720e0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"\\n\".join(table)\n",
    "table_path = Path.cwd().parent / 'tables' / 'eval_summary.tex'\n",
    "table_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(table_path, 'w') as f:\n",
    "    f.write(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
