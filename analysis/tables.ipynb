{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a8baac",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae804c65",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c2f66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbc63bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'codegen-350M-multi': 'CodeGen-350M-multi',\n",
    "    'codegen2-1B_P': 'CodeGen2-1B',\n",
    "    'starcoder2-3b': 'StarCoder2-3B',\n",
    "    'Qwen2.5-Coder-3B': 'Qwen2.5-Coder-3B',\n",
    "    'codegen2-3_7B_P': 'CodeGen2-3.7B',\n",
    "    'CodeLlama-7b-hf': 'CodeLlama-7B',\n",
    "    'codegen2-7B_P': 'CodeGen2-7B',\n",
    "    'starcoder2-7b': 'StarCoder2-7B',\n",
    "    'Qwen2.5-Coder-7B': 'Qwen2.5-Coder-7B',\n",
    "    'Qwen2.5-Coder-14B': 'Qwen2.5-Coder-14B',\n",
    "    'starcoderbase': 'StarCoderBase',\n",
    "    'starcoder2-15b': 'StarCoder2-15B',\n",
    "    'codegen2-16B_P': 'CodeGen2-16B',\n",
    "}\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'methods2test_runnable': '\\\\textsc{Methods2Test\\\\textsubscript{runnable}}',\n",
    "    'humaneval-x': '\\\\textsc{HumanEval-X\\\\textsubscript{java}}',\n",
    "}\n",
    "\n",
    "columns = {\"methods2test_runnable\": ['valid_syntax', 'codebleu_scores', 'passing_rate', 'coverage_instruction', 'coverage_branch', 'mutation_score'],\n",
    "           \"humaneval-x\": ['valid_syntax', 'codebleu_scores', 'passing_rate']}\n",
    "\n",
    "methods = {\n",
    "    'pre-trained': 'None',\n",
    "    'fine-tuning': 'Fine-tuning',\n",
    "    'lora': 'LoRA',\n",
    "    'ia3': '(IA)\\\\textsuperscript{3}',\n",
    "    'prompt-tuning': 'Prompt tuning',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444cf47f",
   "metadata": {},
   "source": [
    "## Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "588eedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainable_params =  pd.read_csv(DATA_DIR / 'params_data.csv', index_col=0)\n",
    "model_trainable_params.index = model_trainable_params.index.str.split('/').str[1]\n",
    "model_trainable_params = model_trainable_params.loc[models.keys()]\n",
    "model_trainable_params[\"pre-trained\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56418cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/methods2test_runnable/valid_syntax.csv\n",
      "                    pre-trained  fine-tuning       ia3      lora  \\\n",
      "model                                                              \n",
      "codegen2-7B_P          0.987090     0.978484  0.984773  0.986759   \n",
      "codegen2-16B_P         0.976167     0.986759  0.977160  0.989738   \n",
      "CodeLlama-7b-hf        0.977424     0.978420  0.980744  0.977424   \n",
      "codegen2-1B_P          0.000000     0.749421  0.019530  0.383979   \n",
      "starcoder2-7b          0.918367     0.969388  0.951942  0.970046   \n",
      "Qwen2.5-Coder-3B       0.969787     0.980624  0.973071  0.978325   \n",
      "codegen2-3_7B_P        0.000000     0.405826  0.000000  0.415094   \n",
      "starcoder2-15b         0.928242     0.979263  0.928242  0.980579   \n",
      "codegen-350M-multi     0.962595     0.980801  0.954651  0.963588   \n",
      "Qwen2.5-Coder-14B      0.978654     0.958621  0.975452  0.971429   \n",
      "starcoder2-3b          0.926926     0.963792  0.940421  0.978934   \n",
      "Qwen2.5-Coder-7B       0.972085     0.943186  0.975041  0.976026   \n",
      "starcoderbase          0.915405     0.973667  0.915405  0.915405   \n",
      "\n",
      "                    prompt-tuning  \n",
      "model                              \n",
      "codegen2-7B_P            0.982787  \n",
      "codegen2-16B_P           0.984442  \n",
      "CodeLlama-7b-hf          0.971116  \n",
      "codegen2-1B_P            0.699106  \n",
      "starcoder2-7b            0.927913  \n",
      "Qwen2.5-Coder-3B         0.967795  \n",
      "codegen2-3_7B_P          0.223105  \n",
      "starcoder2-15b           0.978604  \n",
      "codegen-350M-multi       0.958954  \n",
      "Qwen2.5-Coder-14B        0.976339  \n",
      "starcoder2-3b            0.932851  \n",
      "Qwen2.5-Coder-7B         0.964509  \n",
      "starcoderbase            0.909480  \n",
      "../data/methods2test_runnable/codebleu_scores.csv\n",
      "                    pre-trained  fine-tuning       ia3  prompt-tuning  \\\n",
      "model                                                                   \n",
      "starcoder2-3b          0.098165     0.225242  0.207472       0.104062   \n",
      "starcoder2-7b          0.108485     0.240202  0.244205       0.253046   \n",
      "starcoder2-15b         0.127253     0.254869  0.127253       0.257385   \n",
      "starcoderbase          0.111030     0.263740  0.111030       0.132633   \n",
      "Qwen2.5-Coder-7B       0.232469     0.191285  0.237900       0.230256   \n",
      "Qwen2.5-Coder-3B       0.212861     0.245116  0.219461       0.219134   \n",
      "Qwen2.5-Coder-14B      0.245201     0.236389  0.255412       0.254783   \n",
      "codegen2-16B_P         0.229374     0.243878  0.227714       0.227481   \n",
      "codegen2-3_7B_P        0.000000     0.071191  0.000000       0.255654   \n",
      "codegen2-7B_P          0.216466     0.229558  0.219990       0.215123   \n",
      "codegen2-1B_P          0.000000     0.104390  0.257128       0.256099   \n",
      "codegen-350M-multi     0.169861     0.217985  0.172725       0.166189   \n",
      "CodeLlama-7b-hf        0.240975     0.241846  0.236843       0.229159   \n",
      "\n",
      "                        lora  \n",
      "model                         \n",
      "starcoder2-3b       0.238025  \n",
      "starcoder2-7b       0.242775  \n",
      "starcoder2-15b      0.261237  \n",
      "starcoderbase       0.111030  \n",
      "Qwen2.5-Coder-7B    0.248350  \n",
      "Qwen2.5-Coder-3B    0.228228  \n",
      "Qwen2.5-Coder-14B   0.260949  \n",
      "codegen2-16B_P      0.253299  \n",
      "codegen2-3_7B_P     0.066291  \n",
      "codegen2-7B_P       0.240617  \n",
      "codegen2-1B_P       0.025982  \n",
      "codegen-350M-multi  0.189087  \n",
      "CodeLlama-7b-hf     0.261362  \n",
      "../data/methods2test_runnable/passing_rate.csv\n",
      "                    pre-trained  fine-tuning       ia3  prompt-tuning  \\\n",
      "model                                                                   \n",
      "starcoder2-3b          0.534611     0.166729  0.265715       0.543138   \n",
      "starcoder2-7b          0.526669     0.196648  0.202047       0.201255   \n",
      "starcoder2-15b         0.525391     0.187731  0.527734       0.179859   \n",
      "starcoderbase          0.478018     0.216841  0.478417       0.337986   \n",
      "Qwen2.5-Coder-7B       0.199555     0.135073  0.236404       0.207094   \n",
      "Qwen2.5-Coder-3B       0.177215     0.150923  0.185778       0.197522   \n",
      "Qwen2.5-Coder-14B      0.238905     0.202637  0.241830       0.201170   \n",
      "codegen2-16B_P         0.178771     0.133210  0.190512       0.188525   \n",
      "codegen2-7B_P          0.175885     0.147791  0.182699       0.194771   \n",
      "codegen-350M-multi     0.093620     0.135165  0.133789       0.122738   \n",
      "CodeLlama-7b-hf        0.179574     0.222928  0.203573       0.110619   \n",
      "codegen2-3_7B_P        0.000000     0.049195  0.000000       0.000000   \n",
      "codegen2-1B_P          0.000000     0.038924  0.000000       0.000000   \n",
      "\n",
      "                        lora  \n",
      "model                         \n",
      "starcoder2-3b       0.187731  \n",
      "starcoder2-7b       0.208808  \n",
      "starcoder2-15b      0.212121  \n",
      "starcoderbase       0.478018  \n",
      "Qwen2.5-Coder-7B    0.230912  \n",
      "Qwen2.5-Coder-3B    0.199408  \n",
      "Qwen2.5-Coder-14B   0.240211  \n",
      "codegen2-16B_P      0.171450  \n",
      "codegen2-7B_P       0.173432  \n",
      "codegen-350M-multi  0.145338  \n",
      "CodeLlama-7b-hf     0.221275  \n",
      "codegen2-3_7B_P     0.118457  \n",
      "codegen2-1B_P       0.120837  \n",
      "../data/methods2test_runnable/coverage_instruction.csv\n",
      "                    pre-trained  fine-tuning       ia3  prompt-tuning  \\\n",
      "model                                                                   \n",
      "starcoder2-3b          0.136637     0.618894  0.411980       0.149816   \n",
      "starcoder2-7b          0.117872     0.603305  0.624373       0.538838   \n",
      "starcoder2-15b         0.168153     0.491341  0.168146       0.615352   \n",
      "starcoderbase          0.153714     0.584006  0.156092       0.201802   \n",
      "Qwen2.5-Coder-7B       0.586825     0.568436  0.644698       0.627114   \n",
      "Qwen2.5-Coder-3B       0.529893     0.529238  0.599359       0.534678   \n",
      "Qwen2.5-Coder-14B      0.577331     0.547077  0.611225       0.591982   \n",
      "codegen2-16B_P         0.566742     0.509084  0.568580       0.608381   \n",
      "codegen2-7B_P          0.566250     0.514210  0.580382       0.538250   \n",
      "codegen-350M-multi     0.521171     0.476571  0.496243       0.431285   \n",
      "CodeLlama-7b-hf        0.604497     0.616125  0.593054       0.589615   \n",
      "codegen2-3_7B_P        0.000000     0.327273  0.000000       0.000000   \n",
      "codegen2-1B_P          0.000000     0.306173  0.000000       0.000000   \n",
      "\n",
      "                        lora  \n",
      "model                         \n",
      "starcoder2-3b       0.589704  \n",
      "starcoder2-7b       0.562930  \n",
      "starcoder2-15b      0.612223  \n",
      "starcoderbase       0.153714  \n",
      "Qwen2.5-Coder-7B    0.624860  \n",
      "Qwen2.5-Coder-3B    0.545023  \n",
      "Qwen2.5-Coder-14B   0.609358  \n",
      "codegen2-16B_P      0.516753  \n",
      "codegen2-7B_P       0.602902  \n",
      "codegen-350M-multi  0.481847  \n",
      "CodeLlama-7b-hf     0.614505  \n",
      "codegen2-3_7B_P     0.089512  \n",
      "codegen2-1B_P       0.007874  \n",
      "../data/methods2test_runnable/coverage_branch.csv\n",
      "                    pre-trained  fine-tuning       ia3  prompt-tuning  \\\n",
      "model                                                                   \n",
      "starcoder2-3b          0.057181     0.215099  0.138121       0.058390   \n",
      "starcoder2-7b          0.051513     0.202819  0.219710       0.191203   \n",
      "starcoder2-15b         0.065691     0.172157  0.066139       0.235301   \n",
      "starcoderbase          0.062272     0.217276  0.063890       0.087881   \n",
      "Qwen2.5-Coder-7B       0.214556     0.183191  0.213026       0.224834   \n",
      "Qwen2.5-Coder-3B       0.191113     0.126771  0.217971       0.201420   \n",
      "Qwen2.5-Coder-14B      0.200865     0.170043  0.196238       0.200171   \n",
      "codegen2-16B_P         0.220951     0.146833  0.208442       0.243081   \n",
      "codegen2-7B_P          0.214785     0.203988  0.220720       0.207218   \n",
      "codegen-350M-multi     0.116509     0.159640  0.128809       0.126208   \n",
      "CodeLlama-7b-hf        0.225405     0.223306  0.239561       0.128741   \n",
      "codegen2-3_7B_P        0.000000     0.145455  0.000000       0.000000   \n",
      "codegen2-1B_P          0.000000     0.092593  0.000000       0.000000   \n",
      "\n",
      "                        lora  \n",
      "model                         \n",
      "starcoder2-3b       0.196544  \n",
      "starcoder2-7b       0.203808  \n",
      "starcoder2-15b      0.223649  \n",
      "starcoderbase       0.062272  \n",
      "Qwen2.5-Coder-7B    0.210554  \n",
      "Qwen2.5-Coder-3B    0.190827  \n",
      "Qwen2.5-Coder-14B   0.205084  \n",
      "codegen2-16B_P      0.188457  \n",
      "codegen2-7B_P       0.211533  \n",
      "codegen-350M-multi  0.145467  \n",
      "CodeLlama-7b-hf     0.236187  \n",
      "codegen2-3_7B_P     0.058140  \n",
      "codegen2-1B_P       0.000000  \n",
      "../data/methods2test_runnable/mutation_score.csv\n",
      "                    pre-trained  fine-tuning       ia3  prompt-tuning  \\\n",
      "model                                                                   \n",
      "starcoder2-3b          0.517052     0.384615  0.392283       0.283830   \n",
      "starcoder2-7b          0.365566     0.142857  0.406020       0.430396   \n",
      "starcoder2-15b         0.374115     0.034483  0.433847       0.516861   \n",
      "starcoderbase          0.277693     0.407800  0.458187       0.245424   \n",
      "Qwen2.5-Coder-7B       0.485355     0.000000  0.387789       0.360893   \n",
      "Qwen2.5-Coder-3B       0.000000     0.000000  0.640334       0.551689   \n",
      "Qwen2.5-Coder-14B      0.404657     0.119048  0.265707       0.537979   \n",
      "codegen2-16B_P         0.704072     0.050000  0.525369       0.538580   \n",
      "codegen2-7B_P          0.352236     0.267045  0.517981       0.509980   \n",
      "codegen-350M-multi     0.227451     0.833333  0.449383       0.442290   \n",
      "CodeLlama-7b-hf        0.138889     0.000000  0.360452       0.187500   \n",
      "codegen2-3_7B_P        0.000000     0.000000  0.000000       0.000000   \n",
      "codegen2-1B_P          0.000000     0.111111  0.000000       0.000000   \n",
      "\n",
      "                        lora  \n",
      "model                         \n",
      "starcoder2-3b       0.528126  \n",
      "starcoder2-7b       0.525865  \n",
      "starcoder2-15b      0.157576  \n",
      "starcoderbase       0.467069  \n",
      "Qwen2.5-Coder-7B    0.512818  \n",
      "Qwen2.5-Coder-3B    0.511409  \n",
      "Qwen2.5-Coder-14B   0.526848  \n",
      "codegen2-16B_P      0.293818  \n",
      "codegen2-7B_P       0.408352  \n",
      "codegen-350M-multi  0.414230  \n",
      "CodeLlama-7b-hf     0.505328  \n",
      "codegen2-3_7B_P     0.576923  \n",
      "codegen2-1B_P       0.742424  \n",
      "../data/humaneval-x/valid_syntax.csv\n",
      "                         ia3      lora  pre-trained  fine-tuning  \\\n",
      "model                                                              \n",
      "starcoderbase       0.987805  0.987805     0.987805     1.000000   \n",
      "Qwen2.5-Coder-14B   1.000000  1.000000     1.000000     1.000000   \n",
      "codegen2-7B_P       1.000000  1.000000     1.000000     1.000000   \n",
      "codegen2-16B_P      1.000000  1.000000     1.000000     0.981707   \n",
      "Qwen2.5-Coder-3B    1.000000  1.000000     1.000000     1.000000   \n",
      "codegen2-1B_P       0.000000  0.085366     0.000000     0.054878   \n",
      "codegen2-3_7B_P     0.000000  0.402439     0.000000     0.737805   \n",
      "CodeLlama-7b-hf     1.000000  0.993902     0.993902     1.000000   \n",
      "starcoder2-3b       1.000000  1.000000     1.000000     1.000000   \n",
      "starcoder2-7b       1.000000  1.000000     1.000000     1.000000   \n",
      "starcoder2-15b      1.000000  1.000000     1.000000     0.993902   \n",
      "codegen-350M-multi  1.000000  1.000000     1.000000     1.000000   \n",
      "Qwen2.5-Coder-7B    1.000000  1.000000     1.000000     1.000000   \n",
      "\n",
      "                    prompt-tuning  \n",
      "model                              \n",
      "starcoderbase            0.780488  \n",
      "Qwen2.5-Coder-14B        1.000000  \n",
      "codegen2-7B_P            1.000000  \n",
      "codegen2-16B_P           1.000000  \n",
      "Qwen2.5-Coder-3B         0.993902  \n",
      "codegen2-1B_P            0.079268  \n",
      "codegen2-3_7B_P          0.000000  \n",
      "CodeLlama-7b-hf          0.993902  \n",
      "starcoder2-3b            1.000000  \n",
      "starcoder2-7b            1.000000  \n",
      "starcoder2-15b           1.000000  \n",
      "codegen-350M-multi       1.000000  \n",
      "Qwen2.5-Coder-7B         1.000000  \n",
      "../data/humaneval-x/codebleu_scores.csv\n",
      "                    pre-trained  fine-tuning       ia3  prompt-tuning  \\\n",
      "model                                                                   \n",
      "CodeLlama-7b-hf        0.395052     0.407549  0.390632       0.368276   \n",
      "starcoder2-3b          0.410889     0.406472  0.422353       0.376188   \n",
      "starcoder2-7b          0.354089     0.432025  0.421053       0.421559   \n",
      "starcoder2-15b         0.336566     0.399711  0.336467       0.437893   \n",
      "starcoderbase          0.397604     0.388590  0.397660       0.220605   \n",
      "Qwen2.5-Coder-7B       0.389902     0.414438  0.394362       0.414637   \n",
      "Qwen2.5-Coder-3B       0.418928     0.392657  0.463295       0.358808   \n",
      "Qwen2.5-Coder-14B      0.374079     0.434910  0.385367       0.389855   \n",
      "codegen2-16B_P         0.385003     0.278890  0.385003       0.374880   \n",
      "codegen2-3_7B_P        0.000000     0.256077  0.000000       0.000000   \n",
      "codegen2-7B_P          0.378537     0.350687  0.390794       0.355511   \n",
      "codegen2-1B_P          0.000000     0.010512  0.000000       0.254733   \n",
      "codegen-350M-multi     0.277267     0.241750  0.276146       0.246024   \n",
      "\n",
      "                        lora  \n",
      "model                         \n",
      "CodeLlama-7b-hf     0.347323  \n",
      "starcoder2-3b       0.320707  \n",
      "starcoder2-7b       0.420426  \n",
      "starcoder2-15b      0.377491  \n",
      "starcoderbase       0.369351  \n",
      "Qwen2.5-Coder-7B    0.356214  \n",
      "Qwen2.5-Coder-3B    0.452750  \n",
      "Qwen2.5-Coder-14B   0.406898  \n",
      "codegen2-16B_P      0.373440  \n",
      "codegen2-3_7B_P     0.041367  \n",
      "codegen2-7B_P       0.367108  \n",
      "codegen2-1B_P       0.007713  \n",
      "codegen-350M-multi  0.290468  \n",
      "../data/humaneval-x/passing_rate.csv\n",
      "                    pre-trained  fine-tuning       ia3  prompt-tuning  \\\n",
      "model                                                                   \n",
      "CodeLlama-7b-hf        0.208589     0.219512  0.213415       0.202454   \n",
      "starcoder2-3b          0.219512     0.189024  0.237805       0.182927   \n",
      "starcoder2-7b          0.231707     0.219512  0.213415       0.243902   \n",
      "starcoder2-15b         0.250000     0.233129  0.250000       0.280488   \n",
      "starcoderbase          0.203704     0.219512  0.203704       0.101562   \n",
      "Qwen2.5-Coder-7B       0.365854     0.176829  0.384146       0.408537   \n",
      "Qwen2.5-Coder-3B       0.341463     0.207317  0.378049       0.312883   \n",
      "Qwen2.5-Coder-14B      0.304878     0.347561  0.335366       0.378049   \n",
      "codegen2-16B_P         0.134146     0.074534  0.134146       0.134146   \n",
      "codegen2-7B_P          0.134146     0.079268  0.128049       0.121951   \n",
      "codegen-350M-multi     0.030488     0.018293  0.030488       0.018293   \n",
      "codegen2-3_7B_P        0.000000     0.000000  0.000000       0.000000   \n",
      "codegen2-1B_P          0.000000     0.000000  0.000000       0.000000   \n",
      "\n",
      "                        lora  \n",
      "model                         \n",
      "CodeLlama-7b-hf     0.202454  \n",
      "starcoder2-3b       0.164634  \n",
      "starcoder2-7b       0.219512  \n",
      "starcoder2-15b      0.256098  \n",
      "starcoderbase       0.197531  \n",
      "Qwen2.5-Coder-7B    0.323171  \n",
      "Qwen2.5-Coder-3B    0.396341  \n",
      "Qwen2.5-Coder-14B   0.384146  \n",
      "codegen2-16B_P      0.140244  \n",
      "codegen2-7B_P       0.121951  \n",
      "codegen-350M-multi  0.018293  \n",
      "codegen2-3_7B_P     0.000000  \n",
      "codegen2-1B_P       0.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    dataset_dir = DATA_DIR / dataset\n",
    "    dataset_data = {}\n",
    "    dataset_columns = []\n",
    "\n",
    "    for name in columns[dataset]:\n",
    "        file_path = dataset_dir / f\"{name}.csv\"\n",
    "        try:\n",
    "            print(file_path)\n",
    "            df = pd.read_csv(file_path, index_col=0)\n",
    "            df.index = df.index.str.split('/').str[1]\n",
    "            print(df)\n",
    "            df = df.loc[models.keys()]\n",
    "            dataset_data[name] = df\n",
    "            dataset_columns.append(name)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Missing file: {file_path} â€” skipping.\")\n",
    "\n",
    "    if dataset_data:\n",
    "        data[dataset] = pd.concat(\n",
    "            [dataset_data[col] for col in dataset_columns],\n",
    "            axis=1,\n",
    "            keys=dataset_columns\n",
    "        )\n",
    "\n",
    "# Combine all datasets (outer concat across dataset names)\n",
    "if data:\n",
    "    data = pd.concat(data, axis=1)\n",
    "    data = data.astype(float).round(2)\n",
    "else:\n",
    "    data = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08418048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/v12j1gk96gz44lcngd9z6ddh0000gn/T/ipykernel_21789/3602500784.py:6: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  model_data = data[dataset, column].loc[model]\n"
     ]
    }
   ],
   "source": [
    "best_method_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for column in columns[dataset]:\n",
    "        for model in models.keys():\n",
    "            model_data = data[dataset, column].loc[model]\n",
    "            # Find max value and what method it corresponds to\n",
    "            max_value = model_data.max()\n",
    "            max_indices = model_data[model_data == max_value].index.tolist()\n",
    "            # If all values are the same, max_indices will be empty\n",
    "            #if set(max_indices) == set(model_data.index.tolist()):\n",
    "            #    print(f\"All values are the same for {dataset}, {column}, {model}.\")\n",
    "            #    continue\n",
    "            \n",
    "            for method_index in max_indices:\n",
    "                best_method_data.setdefault(model, [])\n",
    "                best_method_data[model].append((dataset, column, method_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a631b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/v12j1gk96gz44lcngd9z6ddh0000gn/T/ipykernel_21789/2546448888.py:6: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  model_data = data[dataset, column].loc[model]\n"
     ]
    }
   ],
   "source": [
    "decreased_performance_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for column in columns[dataset]:\n",
    "        for model in models.keys():\n",
    "            model_data = data[dataset, column].loc[model]\n",
    "            # Find max value and what method it corresponds to\n",
    "            \n",
    "            baseline_value = model_data.loc['pre-trained']\n",
    "            \n",
    "            decreased_indices = model_data[model_data < baseline_value].index.tolist()\n",
    "            \n",
    "            for method_index in decreased_indices:\n",
    "                decreased_performance_data.setdefault(model, [])\n",
    "                decreased_performance_data[model].append((dataset, column, method_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55bf7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_syntactical_performance_data = {}\n",
    "\n",
    "for dataset in datasets.keys():\n",
    "    for method in methods.keys():\n",
    "        for model in models.keys():\n",
    "            valid_syntax = data[dataset, \"valid_syntax\", method].loc[model]\n",
    "            if valid_syntax < 0.5:\n",
    "                for column in columns[dataset]:  # Skip 'valid_syntax'\n",
    "                    bad_syntactical_performance_data.setdefault(model, [])\n",
    "                    bad_syntactical_performance_data[model].append((dataset, column, method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ae2776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_latex(text):\n",
    "    \"\"\"Escape LaTeX special characters.\"\"\"\n",
    "    return text.replace('_', '\\\\_').replace('%', '\\\\%').replace('&', '\\\\&').replace('$', '\\\\$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61900ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_letter(n):\n",
    "    return chr(ord('a') + n - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d702838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "methods2test_runnable\n",
      "Adding dataset methods2test_runnable with 6 columns\n",
      "humaneval-x\n",
      "0\n",
      "OKOKOKOk\n",
      "methods2test_runnable\n",
      "Adding dataset methods2test_runnable with 6 columns\n",
      "humaneval-x\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "table = []\n",
    "\n",
    "#table.append(\"\\\\begin{table*}[htbp]\")\n",
    "table.append(\"\\\\begin{ThreePartTable}\")\n",
    "table.append(\"    \\\\newcolumntype{Y}{>{\\\\centering\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\newcolumntype{R}{>{\\\\raggedright\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\newcolumntype{L}{>{\\\\raggedleft\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\centering\")\n",
    "table.append(\"    \\\\renewcommand{\\\\arraystretch}{1.25}\")\n",
    "table.append(\"    \\\\footnotesize\")\n",
    "#table.append(\"    \\\\caption{Evaluation metrics experiment results using different tuning methods across various models.}\\\\label{tab:eval-summary}\")\n",
    "\n",
    "table.append(\"    \\\\begin{TableNotes}[flushleft, para]\\\\small\")\n",
    "table.append(\"      \\\\item \\\\textbf{Bold}: best-performing training method per model. (Parentheses): decreased performance compared to baseline. \\\\colorbox{red!10}{Red}: $<$ 50\\\\% syntactical valid samples.\")\n",
    "table.append(\"    \\\\end{TableNotes}\")\n",
    "\n",
    "row = \"    \\\\begin{xltabular}{\\\\textwidth}{lr!{\\\\color{white}\\\\hspace{.5em}}\"\n",
    "\n",
    "for i, dataset in enumerate(datasets, start=1):\n",
    "    \n",
    "    row += \"Y\" * len(columns[dataset])  # 5 columns for each dataset\n",
    "    if i < len(datasets):\n",
    "        row += \"!{\\\\color{white}\\\\hspace{1em}}\"\n",
    "row += \"}\"\n",
    "table.append(row)\n",
    "\n",
    "for i in range(2):\n",
    "    print\n",
    "    if i == 0:\n",
    "        table.append(\"    \\\\caption{Evaluation metrics experiment results using different tuning methods across various models.}\\\\label{tab:eval-summary}\\\\\\\\\")\n",
    "    else:\n",
    "        table.append(\"            \\\\caption{(continued) Evaluation metrics experiment results using different tuning methods across various models.}\\\\\\\\\")\n",
    "    \n",
    "    #table.append(\"    \\\\begin{tabularx}{\\\\textwidth}{lrLLLLL!{\\\\color{white}\\\\ }LLLLL}\")\n",
    "    #\\multicolumn{2}{c}{\\normalsize\\textbf{(a)}} & \\multicolumn{5}{c}{\\normalsize\\textbf{(b)}} & \\multicolumn{3}{c}{\\normalsize\\textbf{(c)}}\\\\[.5em]\n",
    "\n",
    "    row = \"        \\\\multicolumn{2}{c}{\\\\normalsize\\\\textbf{(a)}} & \"\n",
    "    for j, dataset in enumerate(datasets, start=2):\n",
    "        print(dataset)\n",
    "        row += \"\\\\multicolumn{\" + str(len(columns[dataset])) + \"}{c}{\\\\normalsize\\\\textbf{(\" + str(int_to_letter(j)) + \")}}\"\n",
    "        if j < len(datasets) + 1:\n",
    "            print(f\"Adding dataset {dataset} with {len(columns[dataset])} columns\")\n",
    "            row += \" & \"\n",
    "    row += \"\\\\\\\\[.5em]\"\n",
    "    table.append(row)\n",
    "\n",
    "    table.append(\"        \\\\cmidrule(lr){1-2}\\\\cmidrule(lr){3-8}\\\\cmidrule(lr){9-11}\")\n",
    "\n",
    "    row = \"        \\\\multirow{2}{*}{\\\\textbf{Method}} & \\\\multirow{2}{*}{\\\\parbox[t]{1cm}{\\\\centering \\\\textbf{Trainable\\\\\\\\params}}}\"\n",
    "\n",
    "    for _, dataset in enumerate(datasets):\n",
    "        row += \" & \\\\multicolumn{\" + str(len(columns[dataset])) + \"}{c}{\\\\textbf{\" + escape_latex(datasets[dataset]) + \"}}\"\n",
    "    #row += \" & \\\\multicolumn{5}{c}{\\\\textbf{\" + datasets[dataset] + \"}}\"\n",
    "    #row += \" & \\\\multicolumn{3}{c}{\\\\textbf{\" + datasets[dataset] + \"}}\"\n",
    "    row += \"\\\\\\\\\"\n",
    "    table.append(row)\n",
    "\n",
    "    #table.append(\"        \\\\hline\")\n",
    "    #\\cmidrule(lr){3-7}\\cmidrule(lr){8-12}\n",
    "    row = \"        \"\n",
    "    index = 3\n",
    "    for _, dataset in enumerate(datasets):\n",
    "        row += \"\\\\cmidrule(lr){\" + f\"{index}-{index + len(columns[dataset])-1}\" + \"}\"\n",
    "        index += len(columns[dataset])\n",
    "    table.append(row)\n",
    "    \n",
    "    row  = \"        &\"\n",
    "    row += \" & \\\\rotatebox[origin=l]{90}{Valid syntax} & \\\\rotatebox[origin=l]{90}{CodeBLEU} & \\\\rotatebox[origin=l]{90}{pass@1} & \\\\rotatebox[origin=l]{90}{Instr. Cov.} & \\\\rotatebox[origin=l]{90}{Branch Cov.} & \\\\rotatebox[origin=l]{90}{Mutation Score}\"\n",
    "    row += \" & \\\\rotatebox[origin=l]{90}{Valid syntax} & \\\\rotatebox[origin=l]{90}{CodeBLEU} & \\\\rotatebox[origin=l]{90}{pass@1}\"\n",
    "    row += \"\\\\\\\\\"\n",
    "    table.append(row)\n",
    "\n",
    "    table.append(\"        \\\\hline\")\n",
    "    \n",
    "    print(i)\n",
    "    if i == 0:\n",
    "        print(\"OKOKOKOk\")\n",
    "        table.append(\"        \\\\endfirsthead\")\n",
    "    else:\n",
    "        table.append(\"        \\\\endhead\")\n",
    "\n",
    "table.append(\"        \\\\bottomrule\")\n",
    "table.append(\"        \\\\multicolumn{11}{r}{to be continued on the next page}\")\n",
    "table.append(\"        \\\\endfoot\")\n",
    "table.append(\"        \\\\bottomrule\")\n",
    "table.append(\"        \\\\insertTableNotes\")\n",
    "table.append(\"        \\\\endlastfoot\")\n",
    "\n",
    "\n",
    "\n",
    "for model in models.keys():\n",
    "    table.append(\"        \\\\multicolumn{\" + str(2+6+3) + \"}{l}{\\\\cellcolor{gray!10}{\\\\textbf{\" + models[model] + \"}}} \\\\bigstrut \\\\\\\\*\")\n",
    "    for method in methods:\n",
    "        col = []\n",
    "        for dataset in datasets:\n",
    "            for column in columns[dataset]:\n",
    "                try:\n",
    "                    value = data[dataset, column, method].loc[model]\n",
    "                except KeyError:\n",
    "                    value = \"N/A\"\n",
    "            \n",
    "                if (dataset, column, method) in decreased_performance_data.get(model, []):\n",
    "                    value = f\"({value})\"\n",
    "                    \n",
    "                if (dataset, column, method) in best_method_data.get(model, []):\n",
    "                    value = f\"\\\\textbf{{{value}}}\"\n",
    "                    \n",
    "                if (dataset, column, method) in bad_syntactical_performance_data.get(model, []):\n",
    "                    value = f\"\\\\cellcolor{{red!10}}{{{value}}}\"\n",
    "                    \n",
    "                col.append(f\"{value}\")\n",
    "        row = \" & \".join(col)\n",
    "        params = model_trainable_params[method].loc[model]\n",
    "        row = \"        \" + methods[method] + \" & \" + params + \" & \" + row + \" \\\\\\\\*\"\n",
    "        table.append(row)\n",
    "    table[-1] = table[-1][:-1]\n",
    "    table.append(\"\")\n",
    "\n",
    "table[-2] = table[-2][:-3]  # Remove last extra newline\n",
    "\n",
    "#table.append(\"       \\\\bottomrule\")\n",
    "table.append(\"    \\\\end{xltabular}\")\n",
    "table.append(\"\\\\end{ThreePartTable}\")\n",
    "#table.append(\"\\\\end{table*}\")\n",
    "#print(\"\\n\".join(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "720e0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"\\n\".join(table)\n",
    "table_path = Path.cwd().parent / 'tables' / 'eval_summary.tex'\n",
    "table_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(table_path, 'w') as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f9880",
   "metadata": {},
   "source": [
    "## Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7469d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"assert\", \"verify\", \"fail\"]\n",
    "\n",
    "def contains_keyword(text):\n",
    "    for keyword in keywords:\n",
    "        if keyword in text.lower():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2aa1af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses = {\n",
    "    \"success\": \"Succ.\",\n",
    "    \"failed\": \"Failed\",\n",
    "    \"error\": \"Interrupt\",\n",
    "    \"compilation error\": \"CompErr\",\n",
    "    \"no_assertions\": \"NoAssert\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fab26764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table = []\n",
    "\n",
    "table.append(\"\\\\begin{table}[htbp]\")\n",
    "table.append(\"    \\\\newcolumntype{Y}{>{\\\\centering\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\centering\")\n",
    "table.append(\"    \\\\caption{Test execution statuses for StarCode2-7B.}\")\n",
    "table.append(\"    \\\\label{tab:test-statuses}\")\n",
    "table.append(\"    \\\\small\")\n",
    "table.append(\"    \\\\begin{tabularx}{\\\\columnwidth}{lYYYYYY}\")\n",
    "table.append(\"    \\\\toprule\")\n",
    "table.append(\"    \\\\textbf{Method} & \\\\textbf{Succ.} & \\\\textbf{Failed} & \\\\textbf{Interrupt} & \\\\textbf{CompErr} & \\\\textbf{NoAssert} \\\\\\\\\")\n",
    "table.append(\"    \\\\midrule\")\n",
    "\n",
    "for method in methods:\n",
    "    col = []\n",
    "    \n",
    "    path = DATA_DIR / \"methods2test_runnable/executed\" / method / \"bigcode/starcoder2-7b/jacoco.jsonl\"\n",
    "    df = pd.read_json(path, lines=True, dtype=False).set_index(\"id\")\n",
    "    df = df[df[\"status\"] != \"exception\"]\n",
    "    \n",
    "    gen_data_path = Path(str(path).replace(\"executed\", \"fixed\").replace(\"jacoco.jsonl\", \"00001-of-00001.jsonl\"))\n",
    "    gen_df = pd.read_json(gen_data_path, lines=True, dtype=False).set_index(\"id\")\n",
    "    ids = gen_df[~gen_df[\"prediction\"].apply(contains_keyword)].index.tolist()\n",
    "    keys = df.index.intersection(ids)\n",
    "    no_assertions_df = df.loc[(df.index.isin(keys)) & (df[\"status\"] == \"success\")]\n",
    "    \n",
    "    values = df['status'].value_counts().sort_index()\n",
    "    values.loc['success'] = values.loc['success'] - no_assertions_df.shape[0]\n",
    "    values.loc['no_assertions'] = no_assertions_df.shape[0]\n",
    "\n",
    "    percentages = (values / values.sum())\n",
    "    \n",
    "    for status in statuses.keys():\n",
    "        value = str(int(percentages.loc[status].round(2) * 100)) + \"\\\\%\"\n",
    "        col.append(value)\n",
    "\n",
    "    \n",
    "    row = \" & \".join(col)\n",
    "    row = methods[method] + \" & \" + row\n",
    "    table.append(\"    \" + row + \" \\\\\\\\\")\n",
    "    \n",
    "table.append(\"    \\\\bottomrule\")\n",
    "table.append(\"    \\\\end{tabularx}\")\n",
    "table.append(\"\\\\end{table}\")\n",
    "\n",
    "#print(\"\\n\".join(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86befb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"\\n\".join(table)\n",
    "table_path = Path.cwd().parent / 'tables' / 'test_execution_example.tex'\n",
    "table_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(table_path, 'w') as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0d196",
   "metadata": {},
   "source": [
    "# Model Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "961ddb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table = []\n",
    "\n",
    "table.append(\"\\\\begin{table}[htbp]\")\n",
    "table.append(\"    \\\\newcolumntype{Y}{>{\\\\centering\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\newcolumntype{R}{>{\\\\raggedleft\\\\arraybackslash}X}\")\n",
    "table.append(\"    \\\\centering\")\n",
    "table.append(\"    \\\\caption{HuggingFace model download statistics from cfahlgren1/hub-stats as of 13 November 2025.}\")\n",
    "table.append(\"    \\\\label{tab:model-downloads}\")\n",
    "table.append(\"    \\\\small\")\n",
    "table.append(\"    \\\\begin{tabularx}{\\\\columnwidth}{lRRY}\")\n",
    "table.append(\"    \\\\toprule\")\n",
    "table.append(\"    \\\\textbf{Model} & \\\\textbf{Downloads last month} & \\\\textbf{Downloads all time} & \\\\textbf{Created at} \\\\\\\\\")\n",
    "table.append(\"    \\\\midrule\")\n",
    "\n",
    "path = DATA_DIR / \"model_downloads.csv\"\n",
    "df = pd.read_csv(path).set_index(\"id\")\n",
    "\n",
    "for model_name, row in df.iterrows():\n",
    "    col = []\n",
    "    \n",
    "    col.append(escape_latex(model_name))\n",
    "    col.append(str(row[\"downloads\"]))\n",
    "    col.append(str(row[\"downloadsAllTime\"]))\n",
    "    col.append(str(row[\"createdAt\"].split()[0]))\n",
    "\n",
    "    row = \" & \".join(col)\n",
    "    table.append(\"    \" + row + \" \\\\\\\\\")\n",
    "    \n",
    "table.append(\"    \\\\bottomrule\")\n",
    "table.append(\"    \\\\end{tabularx}\")\n",
    "table.append(\"\\\\end{table}\")\n",
    "\n",
    "#print(\"\\n\".join(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "218e0c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"\\n\".join(table)\n",
    "table_path = Path.cwd().parent / 'tables' / 'model_downloads.tex'\n",
    "table_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(table_path, 'w') as f:\n",
    "    f.write(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
